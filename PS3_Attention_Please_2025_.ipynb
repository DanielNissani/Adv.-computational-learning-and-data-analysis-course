{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "# Neural Machine Translation with Attention\n",
        "\n",
        "Advanced Learning Fall 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For SUBMISSION:   \n",
        "\n",
        "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
        "\n",
        "~~~\n",
        "STUDENT ID: MISSING\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "STUDENT GIT LINK: MISSING\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files, and upload to moodle the html version:    \n",
        "  \n",
        "`PS3_Attention_2025_ID_[000000000].html`   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PpJdYve9cZa6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecp2PAf7qJq"
      },
      "source": [
        "In this problem set we are going to jump into the depths of `seq2seq` and `attention` and build a couple of PyTorch translation mechanisms with some  twists.     \n",
        "\n",
        "\n",
        "*   Part 1 consists of a somewhat unorthodox `seq2seq` model for simple arithmetics\n",
        "*   Part 2 consists of an `seq2seq - attention` language translation model. We will use it for Hebrew and English.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-VpUCez9gOZn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNDsL5HlZN6"
      },
      "source": [
        "A **seq2seq** model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.\n",
        "\n",
        "Here's a breakdown of how `seq2seq` models work:\n",
        "\n",
        "* The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.\n",
        "\n",
        "* information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.\n",
        "\n",
        "* Attention mechanism (optional): Some `seq2seq` models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.\n",
        "\n",
        "`seq2seq` models are used in many natural language processing (NLP) tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbUDn4FObol7"
      },
      "source": [
        "imports: (feel free to add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "## Part 1: Seq2Seq Arithmetic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gWov3Gx67I"
      },
      "source": [
        "**Using RNN `seq2seq` model to \"learn\" simple arithmetics!**\n",
        "\n",
        "> Given the string \"54-7\", the model should return a prediction: \"47\".  \n",
        "> Given the string \"10+20\", the model should return a prediction: \"30\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxo92ZgTy6ED"
      },
      "source": [
        "- Watch Lukas Biewald's short [video](https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1) explaining `seq2seq` models and his toy application (somewhat outdated).\n",
        "- You can find the code for his example [here](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py).    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEu_5YvqFPai"
      },
      "source": [
        "1.1) Using Lukas' code, implement a `seq2seq` network that can learn how to solve **addition AND substraction** of two numbers of maximum length of 4, using the following steps (similar to the example):      \n",
        "\n",
        "* Generate data; X: queries (two numbers), and Y: answers   \n",
        "* One-hot encode X and Y,\n",
        "* Build a `seq2seq` network (with LSTM, RepeatVector, and TimeDistributed layers)\n",
        "* Train the model.\n",
        "* While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.    \n",
        "\n",
        "Notes:  \n",
        "* The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the \"correct\" output - this will help you fix the unsupported \"model.predict_classes\".\n",
        "* Please use the parameters in the code cell below to train the model.     \n",
        "* Instead of using a `wandb.config` object, please use a simple dictionary instead.   \n",
        "* You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.\n",
        "* Extra credit if you can implement the network in PyTorch (this is not difficult).    \n",
        "* Extra credit if you are able to significantly improve the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries"
      ],
      "metadata": {
        "id": "qlMWnOKvQOj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3So3FCDOLNQ",
        "outputId": "ee58089b-9c25-435a-b9f2-aff9ddfaeaf2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data set generation:\n",
        "\n"
      ],
      "metadata": {
        "id": "DeMMqbv3Q4xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Chartable:\n",
        "    # Helper class that maps characters to indices and can encode strings as one-hot vectors (or as index vectors).\n",
        "    def __init__(self, chars: str):\n",
        "        self.chars = chars\n",
        "        self.char2idx = {c: i for i, c in enumerate(chars)}\n",
        "        self.idx2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.chars)\n",
        "\n",
        "    def encode_onehot(self, s: str, max_len: int) -> np.ndarray:\n",
        "        s = (s + \" \" * max(0, max_len - len(s)))[:max_len]\n",
        "        x = np.zeros((max_len, self.vocab_size), dtype=np.float32)\n",
        "        for t, ch in enumerate(s):\n",
        "            x[t, self.char2idx[ch]] = 1.0\n",
        "        return x\n",
        "\n",
        "    def encode_indices(self, s: str, max_len: int) -> np.ndarray:\n",
        "        s = (s + \" \" * max(0, max_len - len(s)))[:max_len]\n",
        "        return np.array([self.char2idx[ch] for ch in s], dtype=np.int64)\n",
        "\n",
        "\n",
        "class Seq2SeqCharDataset(Dataset):\n",
        "    # Returns (x_onehot, y_indices)\n",
        "    def __init__(self, X_onehot: np.ndarray, Y_indices: np.ndarray):\n",
        "        self.X = torch.from_numpy(X_onehot)   # (N, T_in, V) float32\n",
        "        self.Y = torch.from_numpy(Y_indices)  # (N, T_out)  int64\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "def gen_dataset(config: dict):\n",
        "    cfg = dict(config) if config is not None else {}\n",
        "    cfg[\"digits\"] = 4\n",
        "    cfg.setdefault(\"training_size\", 40000)\n",
        "    cfg.setdefault(\"batch_size\", 128)\n",
        "    cfg.setdefault(\"seed\", 0)\n",
        "    cfg.setdefault(\"operators\", \"+-\")\n",
        "\n",
        "    rng = np.random.default_rng(cfg[\"seed\"])\n",
        "\n",
        "    digits = cfg[\"digits\"]\n",
        "    maxlen = digits + 1 + digits   # 9\n",
        "    outlen = digits + 1            # 5\n",
        "\n",
        "    chars = \"0123456789+- \"\n",
        "    ctable = Chartable(chars)\n",
        "\n",
        "    questions, expected = [], []\n",
        "    seen = set()\n",
        "\n",
        "    def sample_int():\n",
        "        L = rng.integers(1, digits + 1)\n",
        "        s = \"\".join(rng.choice(list(\"0123456789\"), size=L))\n",
        "        return int(s)\n",
        "\n",
        "    ops = list(cfg[\"operators\"])\n",
        "    if not ops:\n",
        "        raise ValueError(\"config['operators'] must contain at least one of '+' or '-'\")\n",
        "\n",
        "    while len(questions) < cfg[\"training_size\"]:\n",
        "        a, b = sample_int(), sample_int()\n",
        "        op = ops[rng.integers(0, len(ops))]\n",
        "\n",
        "        # avoid duplicates: + is commutative, - is not\n",
        "        if op == \"+\":\n",
        "            key = (\"+\",) + tuple(sorted((a, b)))\n",
        "        else:\n",
        "            key = (\"-\", a, b)\n",
        "\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "\n",
        "        q = f\"{a}{op}{b}\"\n",
        "        if len(q) > maxlen:\n",
        "            continue\n",
        "\n",
        "        ans = str(a + b) if op == \"+\" else str(a - b)\n",
        "        if len(ans) > outlen:\n",
        "            continue\n",
        "\n",
        "        query = q + \" \" * (maxlen - len(q))\n",
        "        ans_padded = ans + \" \" * (outlen - len(ans))\n",
        "\n",
        "        questions.append(query)\n",
        "        expected.append(ans_padded)\n",
        "\n",
        "    # shuffle\n",
        "    perm = rng.permutation(len(questions))\n",
        "    questions = [questions[i] for i in perm]\n",
        "    expected  = [expected[i] for i in perm]\n",
        "\n",
        "    # vectorize\n",
        "    N = len(questions)\n",
        "    X = np.zeros((N, maxlen, ctable.vocab_size), dtype=np.float32)\n",
        "    Y = np.zeros((N, outlen), dtype=np.int64)\n",
        "\n",
        "    for i, (q, a) in enumerate(zip(questions, expected)):\n",
        "        X[i] = ctable.encode_onehot(q, max_len=maxlen)\n",
        "        Y[i] = ctable.encode_indices(a, max_len=outlen)\n",
        "\n",
        "    # split 90/10\n",
        "    n_train = int(0.9 * N)\n",
        "    X_train, Y_train = X[:n_train], Y[:n_train]\n",
        "    X_test,  Y_test  = X[n_train:], Y[n_train:]\n",
        "\n",
        "    trainset = Seq2SeqCharDataset(X_train, Y_train)\n",
        "    testset  = Seq2SeqCharDataset(X_test,  Y_test)\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
        "    test_loader  = DataLoader(testset,  batch_size=cfg[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    meta = {\n",
        "        \"config\": cfg,\n",
        "        \"maxlen\": maxlen,\n",
        "        \"outlen\": outlen,\n",
        "        \"chars\": chars,\n",
        "        \"vocab_size\": ctable.vocab_size,\n",
        "        \"char2idx\": ctable.char2idx,\n",
        "        \"idx2char\": ctable.idx2char,\n",
        "        \"ctable\": ctable,\n",
        "    }\n",
        "\n",
        "    return trainset, testset, train_loader, test_loader, meta"
      ],
      "metadata": {
        "id": "dhtOjZ7VRU3O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Archtiecture"
      ],
      "metadata": {
        "id": "i61HuSercRWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pytorch_LSTM_seq2seq(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, outlen):\n",
        "    super().__init__()\n",
        "    self.outlen = outlen\n",
        "\n",
        "    self.encoder = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
        "    self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    # softmax layer is implemented in the cross-entropy loss\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Encode\n",
        "    enc_out, (h, c) = self.encoder(x) # h, c: (num_layers * num_directions, B, H)\n",
        "\n",
        "    # Take final layer's hidden state as the context vector z\n",
        "    z = h[-1]  # (B, H)\n",
        "\n",
        "    # RepeatVector: make decoder input by repeating z for T_out steps\n",
        "    T_out = self.outlen\n",
        "    dec_in = z.unsqueeze(1).repeat(1, T_out, 1)  # (B, T_out, H)\n",
        "\n",
        "    # Decode (initialize decoder with encoder states)\n",
        "    dec_out, _ = self.decoder(dec_in, (h, c))  # dec_out: (B, T_out, H)\n",
        "\n",
        "    # TimeDistributed(Dense): project each time step to vocab logits\n",
        "    logits = self.fc(dec_out)  # (B, T_out, V)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "NY7GE6RbcQ7P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training loop"
      ],
      "metadata": {
        "id": "43N5BbcO6mh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, ctable, config, device=None):\n",
        "\n",
        "  if device is None:\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  lr = config.get(\"lr\", 1e-3)\n",
        "  epochs = config.get(\"iterations\", 50)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()  # expects logits + class indices\n",
        "\n",
        "# ---------- tiny decoding helpers - for printing ----------\n",
        "  def decode_x(x_onehot):  # (T_in, V)\n",
        "    idx = x_onehot.argmax(dim=-1).tolist()\n",
        "    return \"\".join(ctable.idx2char[i] for i in idx)\n",
        "\n",
        "  def decode_y(y_idx):     # (T_out,)\n",
        "    if torch.is_tensor(y_idx):\n",
        "        y_idx = y_idx.tolist()\n",
        "    return \"\".join(ctable.idx2char[i] for i in y_idx)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    exact_match = 0\n",
        "    total_seqs = 0\n",
        "\n",
        "    for x, y in val_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)  # (B, T_out)\n",
        "\n",
        "      logits = model(x)  # (B, T_out, V)\n",
        "      B, T_out, V = logits.shape\n",
        "\n",
        "      loss = criterion(logits.reshape(B * T_out, V), y.reshape(B * T_out))\n",
        "      total_loss += loss.item() * (B * T_out)\n",
        "      total_tokens += (B * T_out)\n",
        "\n",
        "      preds = logits.argmax(dim=-1)  # (B, T_out)\n",
        "      exact_match += (preds == y).all(dim=1).sum().item()\n",
        "      total_seqs += B\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_tokens)\n",
        "    seq_acc = exact_match / max(1, total_seqs)\n",
        "    model.train()\n",
        "    return avg_loss, seq_acc\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def print_random_val_samples(k=10):\n",
        "    model.eval()\n",
        "    ds = val_loader.dataset\n",
        "    k = min(k, len(ds))\n",
        "\n",
        "    for _ in range(k):\n",
        "      i = random.randrange(len(ds))\n",
        "      x, y = ds[i]  # x: (T_in,V), y: (T_out,)\n",
        "      x_b = x.unsqueeze(0).to(device)# (1,T_in,V)\n",
        "\n",
        "      logits = model(x_b) # (1,T_out,V)\n",
        "      pred = logits.argmax(dim=-1)[0].cpu()  # (T_out,)\n",
        "\n",
        "      q = decode_x(x)\n",
        "      t = decode_y(y)\n",
        "      g = decode_y(pred)\n",
        "\n",
        "      mark = \"V\" if t == g else \"X\"\n",
        "      delta = abs(int(t.strip()) - int(g.strip()))\n",
        "      print(f\"Q {q}  T {t}  {mark}  {g}  Delta = {delta}\")\n",
        "\n",
        "    model.train()\n",
        "    return\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_tokens = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)  # (B, T_out)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(x)  # (B, T_out, V)\n",
        "\n",
        "      B, T_out, V = logits.shape\n",
        "      loss = criterion(logits.reshape(B * T_out, V), y.reshape(B * T_out))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() * (B * T_out)\n",
        "      running_tokens += (B * T_out)\n",
        "\n",
        "    train_loss = running_loss / max(1, running_tokens)\n",
        "    val_loss, val_seq_acc = eval_epoch()\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_exact_seq_acc={val_seq_acc:.3f}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print_random_val_samples(k=10)\n"
      ],
      "metadata": {
        "id": "zVTjQxAR6qL-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main"
      ],
      "metadata": {
        "id": "1JURFKz78MKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"training_size\": 40000,\n",
        "    \"digits\": 4,\n",
        "    \"hidden_size\": 128,\n",
        "    \"batch_size\": 128,\n",
        "    \"iterations\": 50,\n",
        "    \"lr\": 1e-2,\n",
        "    \"seed\": 0,\n",
        "    \"operators\": \"+-\",\n",
        "}\n",
        "\n",
        "# 1) data\n",
        "trainset, valset, train_loader, val_loader, meta = gen_dataset(config)\n",
        "ctable = meta[\"ctable\"]\n",
        "\n",
        "# 2) model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Pytorch_LSTM_seq2seq(\n",
        "    vocab_size=meta[\"vocab_size\"],\n",
        "    hidden_size=config[\"hidden_size\"],\n",
        "    outlen=meta[\"outlen\"],\n",
        ").to(device)\n",
        "\n",
        "# 3) train\n",
        "#train(model, train_loader, val_loader, ctable, config, device=device)"
      ],
      "metadata": {
        "id": "t-3xpTl08Pkg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJQqZbEbRup"
      },
      "source": [
        "###1.2).\n",
        "\n",
        "a) Do you think this model performs well?  Why or why not?     \n",
        "b) What are its limitations?   \n",
        "c) What would you do to improve it?    \n",
        "d) Can you apply an attention mechanism to this model? Why or why not?   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers** **1.2**:\n",
        "\n",
        "a+b. Overall, the model does not perform well: even after training it gets only ~50% of validation sequences exactly correct, meaning half the arithmetic expressions are still wrong. Early on it was worse (20–30%), and increasing the learning rate from 0.001 to 0.01 helped optimization and improved accuracy, but it still doesn’t reach reliable performance.\n",
        "\n",
        "Its main limitation is insufficient context for multi-digit queries. In this architecture the encoder compresses the entire input into a single fixed-size vector, and the decoder generates the whole answer from that (via repeat vector) without attention or explicitly using previous output tokens. This makes it hard to handle operations that require global, structured dependencies like carry/borrow across digits, which becomes more common as the input strings get longer. In the epoch 50 example shared, the model tends to be correct on shorter queries (about 4–5 characters) and fails more on longer ones (about 5-8 characters), which is consistent with the limitations of this architecture and difficulty of preserving detailed context in a single vector.\n",
        "\n",
        "c. The model can be improved in several ways.\n",
        "First, the encoder and decoder LSTMs can be made deeper (using multiple layers) and tuned with a larger or better-chosen hidden dimension. This would increase the model’s capacity to represent complex patterns in the input and output sequences.\n",
        "Second, adding an attention mechanism would significantly improve performance by allowing the decoder to directly focus on relevant parts of the input sequence at each output time step, instead of relying on a single fixed-size context vector. This would help the model handle longer strings and correctly propagate carry and borrow operations across digits.\n",
        "\n",
        "d. Attention requires two building blocks: an encoder that produces a hidden state for each input time step, and a decoder that generates the output sequence step by step. The attention mechanism connects these two by computing, at each decoder time step, a weighted combination of the encoder hidden states. This allows the decoder to focus only on the most relevant parts of the input when predicting the next output token, instead of relying on a single fixed-size context vector."
      ],
      "metadata": {
        "id": "on2OtxB0FFUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3).  \n",
        "\n",
        "Add attention to the model. Evaluate the performance against the `seq2seq` you trained above. Which one is performing better?"
      ],
      "metadata": {
        "id": "6wvRhhOcgmrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn_seq2seq(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, outlen):\n",
        "      super().__init__()\n",
        "      self.vocab_size = vocab_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.outlen = outlen\n",
        "\n",
        "      self.encoder = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
        "      self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "      self.fc = nn.Linear(2 * hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Encode\n",
        "      enc_out, (h, c) = self.encoder(x)\n",
        "      z = h[-1]\n",
        "\n",
        "      # repeat vector -> decoder input\n",
        "      dec_in = z.unsqueeze(1).repeat(1, self.outlen, 1)\n",
        "      dec_out, _ = self.decoder(dec_in, (h, c))\n",
        "\n",
        "      # ----- Attention)-----\n",
        "      # scores[b, t, i] = <dec_out[b,t], enc_out[b,i]>\n",
        "      scores = torch.bmm(dec_out, enc_out.transpose(1, 2))\n",
        "      attn_w = F.softmax(scores, dim=-1)\n",
        "      context = torch.bmm(attn_w, enc_out)\n",
        "\n",
        "      fused = torch.cat([dec_out, context], dim=-1)\n",
        "      logits = self.fc(fused)\n",
        "\n",
        "      return logits\n",
        "\n",
        "config = {\n",
        "    \"training_size\": 40000,\n",
        "    \"digits\": 4,\n",
        "    \"hidden_size\": 128,\n",
        "    \"batch_size\": 128,\n",
        "    \"iterations\": 50,\n",
        "    \"lr\": 1e-2,\n",
        "    \"seed\": 0,\n",
        "    \"operators\": \"+-\",\n",
        "}\n",
        "\n",
        "trainset, valset, train_loader, val_loader, meta = gen_dataset(config)\n",
        "ctable = meta[\"ctable\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Attn_seq2seq(\n",
        "    vocab_size=meta[\"vocab_size\"],\n",
        "    hidden_size=config[\"hidden_size\"],\n",
        "    outlen=meta[\"outlen\"],\n",
        ").to(device)\n",
        "\n",
        "# 3) train\n",
        "#train(model, train_loader, val_loader, ctable, config, device=device)"
      ],
      "metadata": {
        "id": "koh6rjExq_Ll"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model I added an attention mechanism. Attention is implemented by computing a similarity score between each decoder hidden state and all encoder hidden states, applying softmax to get attention weights, and then taking a weighted sum of encoder states to form a context vector. This context vector is concatenated with the decoder output at each step and projected to vocabulary logits.\n",
        "\n",
        "Compared to the previous seq2seq baseline, the attention model performs better: it achieves val_exact_seq_acc = 0.672 (vs ~0.50 for the baseline). Qualitatively, it also handles longer queries better, where the baseline tended to fail (likely because attention reduces the single-vector bottleneck and lets the decoder access relevant parts of the input at each output step)."
      ],
      "metadata": {
        "id": "Q0YCBubuvseR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.4)\n",
        "\n",
        "Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ],
      "metadata": {
        "id": "AtEJK5IZkk8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bwZKyzoBKl4G"
      },
      "outputs": [],
      "source": [
        "class Attn_seq2seq(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, outlen):\n",
        "      super().__init__()\n",
        "      self.vocab_size = vocab_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.outlen = outlen\n",
        "\n",
        "      self.encoder = nn.LSTM(vocab_size, hidden_size, num_layers=2, dropout=0.2, batch_first=True)\n",
        "      self.decoder = nn.LSTM(hidden_size, hidden_size,num_layers=2, dropout=0.2,batch_first=True)\n",
        "      self.fc = nn.Linear(2 * hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Encode\n",
        "      enc_out, (h, c) = self.encoder(x)\n",
        "      z = h[-1]\n",
        "\n",
        "      # repeat vector -> decoder input\n",
        "      dec_in = z.unsqueeze(1).repeat(1, self.outlen, 1)\n",
        "      dec_out, _ = self.decoder(dec_in, (h, c))\n",
        "\n",
        "      # ----- Attention)-----\n",
        "      # scores[b, t, i] = <dec_out[b,t], enc_out[b,i]>\n",
        "      scores = torch.bmm(dec_out, enc_out.transpose(1, 2))\n",
        "      attn_w = F.softmax(scores, dim=-1)\n",
        "      context = torch.bmm(attn_w, enc_out)\n",
        "\n",
        "      fused = torch.cat([dec_out, context], dim=-1)\n",
        "      logits = self.fc(fused)\n",
        "\n",
        "      return logits\n",
        "\n",
        "config = {\n",
        "    \"training_size\": 40000,\n",
        "    \"digits\": 4,\n",
        "    \"hidden_size\": 128,\n",
        "    \"batch_size\": 128,\n",
        "    \"iterations\": 50,\n",
        "    \"lr\": 1e-2,\n",
        "    \"seed\": 0,\n",
        "    \"operators\": \"+-\",\n",
        "}\n",
        "\n",
        "trainset, valset, train_loader, val_loader, meta = gen_dataset(config)\n",
        "ctable = meta[\"ctable\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Attn_seq2seq(\n",
        "    vocab_size=meta[\"vocab_size\"],\n",
        "    hidden_size=config[\"hidden_size\"],\n",
        "    outlen=meta[\"outlen\"],\n",
        ").to(device)\n",
        "\n",
        "# 3) train\n",
        "#train(model, train_loader, val_loader, ctable, config, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note - Accuracy measure: for all models the accuracy measure - \"val_exact_seq_acc\" is measured on all validation set (10%) and not only the 10 queries represented.\n",
        "\n",
        "The model I used is a small modification of the attention model from 1.3. I made the encoder and decoder deeper by increasing the number of LSTM layers from 1 to 2, and added dropout = 0.2 between the stacked LSTM layers. Dropout is a regularization technique that randomly zeros a fraction of activations during training, which helps reduce overfitting.\n",
        "\n",
        "This change led to a large improvement in performance: the baseline seq2seq model achieved about ~50% exact-sequence validation accuracy, the attention model in 1.3 achieved about ~70%, and the deeper attention model achieved about ~80%. I believe the deeper architecture helps because it increases the model’s capacity to represent multi-digit dependencies (especially carry/borrow patterns), while dropout improves generalization."
      ],
      "metadata": {
        "id": "A14ubUjJ2E2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "voVYROYNlO49"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-d0eIM6FeaM"
      },
      "source": [
        "## Part 2: A language translation model with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80jhFbWPMW_a"
      },
      "source": [
        "In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgL38lJGTYaF"
      },
      "source": [
        "0) Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.a) Using `!wget`, `!unzip` , download and extract the [hebrew-english](https://www.manythings.org/anki/) sentence pairs text file to the Colab `content/`  folder (or local folder if not using Colab).\n",
        "1.b) The `heb.txt` must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).   \n"
      ],
      "metadata": {
        "id": "KBX873GJlDl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same `eng_prefixes` filter to limit the train/test data.   \n",
        "2.b) Evaluate your trained model randomly on 20 sentences.  \n",
        "2.c) Show the attention plot for 5 random sentences.  \n"
      ],
      "metadata": {
        "id": "AvIIlNvPlGWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?  \n"
      ],
      "metadata": {
        "id": "qcqtVxkclIWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ],
      "metadata": {
        "id": "i2VSrRNtlJub"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9-C4pLEXzCF"
      },
      "source": [
        "##SOLUTION:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0."
      ],
      "metadata": {
        "id": "W7Ci1Ur4LgLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau attention computes alignment scores using a learned feed-forward network that combines the decoder hidden state with each encoder hidden state through a non-linear transformation. This means the compatibility between states is modeled explicitly and does not rely on vector similarity alone. As a result, the attention distribution is often smoother, since scores are produced by a flexible function rather than a direct similarity measure.\n",
        "\n",
        "Luong attention computes alignment scores using a dot product between encoder and decoder hidden states, directly measuring their similarity in the same vector space. This creates a stronger inductive bias toward one-to-one alignments. The method is simpler and more constrained, but this constraint can lead to clearer and more stable alignments when encoder and decoder representations are well matched.\n",
        "\n",
        "In addition, Bahdanau attention does not require the encoder and decoder hidden states to have the same dimensionality, since they are projected through learned linear layers before comparison, whereas Luong attention assumes compatible dimensions for the dot-product operation. Consequently, Bahdanau attention is more flexible but computationally heavier, while Luong attention is more efficient and often yields sharper alignments in practice.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bMY4cDWNrtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries"
      ],
      "metadata": {
        "id": "nqcikDJotuD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import zipfile, os\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "zip_path = \"/content/drive/MyDrive/deep_learning_course/ex3/heb-eng.zip\"\n",
        "!unzip \"$zip_path\" -d /content/data\n",
        "\n",
        "with open(\"/content/data/heb.txt\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline())\n"
      ],
      "metadata": {
        "id": "_WrHkLD6p813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f1719a-ca5f-43e5-e0fd-dcbffc590516"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/deep_learning_course/ex3/heb-eng.zip\n",
            "replace /content/data/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: מ\n",
            "error:  invalid response [מ]\n",
            "replace /content/data/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/data/heb.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Go.\tלך!\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9435252 (Alkrasnov)\n",
            "\n",
            "Hi.\tהיי.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #694680 (Eldad)\n",
            "\n",
            "Hi.\tאהלן.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #2184365 (MrShoval)\n",
            "\n",
            "Run!\tרוץ!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2805053 (Eldad)\n",
            "\n",
            "Run!\tרוצי!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2805054 (Eldad)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.a. Data preperations"
      ],
      "metadata": {
        "id": "T80C2R8Hx9U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            if word:\n",
        "                self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "# Keep this for English if you want it, but we will NOT apply it to Hebrew.\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Hebrew-safe normalization: lower/trim, normalize whitespace, keep Unicode chars\n",
        "def normalizeAny(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # separate punctuation a bit\n",
        "    s = re.sub(r\"\\s+\", \" \", s) # collapse multiple spaces/tabs\n",
        "    return s\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is \", \"he s \",\n",
        "    \"she is \", \"she s \",\n",
        "    \"you are \", \"you re \",\n",
        "    \"we are \", \"we re \",\n",
        "    \"they are \", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return (\n",
        "        len(p[0].split()) < MAX_LENGTH and\n",
        "        len(p[1].split()) < MAX_LENGTH and\n",
        "        p[1].startswith(eng_prefixes)\n",
        "    )\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False, filepath=\"data/heb.txt\"):\n",
        "    \"\"\"\n",
        "    Reads your extracted /content/data/heb.txt which has 3 tab-separated columns:\n",
        "      English \\t Hebrew \\t attribution/meta\n",
        "\n",
        "    We keep only columns 0 and 1, ignore column 2.\n",
        "    \"\"\"\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    lines = open(filepath, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        cols = l.split(\"\\t\")\n",
        "        if len(cols) < 2:\n",
        "            continue\n",
        "\n",
        "        eng = normalizeAny(unicodeToAscii(cols[0]))\n",
        "        heb = normalizeAny(cols[1])\n",
        "\n",
        "        # skip empty lines after normalization\n",
        "        if not eng or not heb:\n",
        "            continue\n",
        "\n",
        "        # By default store as [lang1, lang2] = [eng, heb]\n",
        "        pairs.append([eng, heb])\n",
        "\n",
        "    # Reverse if you want Hebrew -> English (so input=heb, output=eng)\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)   # heb\n",
        "        output_lang = Lang(lang1)  # eng\n",
        "    else:\n",
        "        input_lang = Lang(lang1)   # eng\n",
        "        output_lang = Lang(lang2)  # heb\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False, filepath=\"data/heb.txt\"):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse=reverse, filepath=filepath)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "# ---- Run: Hebrew -> English (matches the tutorial style where reverse=True) ----\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'heb', reverse=True, filepath=\"data/heb.txt\")\n",
        "print(\"Example pair:\", random.choice(pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXb5Y8yryCXV",
        "outputId": "050a4f48-0fc2-45a0-87f6-21b7cf2bf924"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 136845 sentence pairs\n",
            "Trimmed to 1570 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "heb 1961\n",
            "eng 1199\n",
            "Example pair: ['היא רגילה למסעות .', 'she is used to traveling .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder"
      ],
      "metadata": {
        "id": "IpLws2FZG9WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "3nGERWLNG_lM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder with attention"
      ],
      "metadata": {
        "id": "Vm5wp84OL7Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "RYxaZoHLWI08"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Train data"
      ],
      "metadata": {
        "id": "Sjjvdc9zWIT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "ZKLMJmX8WSRs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper functions"
      ],
      "metadata": {
        "id": "EOru_74aXA9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "XPTF9SphXAhT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plots evaluations and visualizations"
      ],
      "metadata": {
        "id": "2uF_R9JAgstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=20):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "def evaluateAccuracy(encoder, decoder, pairs, input_lang, output_lang, n=None):\n",
        "    \"\"\"\n",
        "    Exact-match accuracy over a list of pairs.\n",
        "    If n is given, evaluates on a random subset of size n.\n",
        "    \"\"\"\n",
        "    if n is not None:\n",
        "        sample = random.sample(pairs, k=min(n, len(pairs)))\n",
        "    else:\n",
        "        sample = pairs\n",
        "\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "\n",
        "    for inp, tgt in sample:\n",
        "        output_words, _ = evaluate(encoder, decoder, inp, input_lang, output_lang)\n",
        "\n",
        "        # remove EOS token if it exists at the end\n",
        "        if len(output_words) > 0 and output_words[-1] == '<EOS>':\n",
        "            output_words = output_words[:-1]\n",
        "\n",
        "        pred_sentence = ' '.join(output_words).strip()\n",
        "        tgt_sentence = tgt.strip()\n",
        "\n",
        "        if pred_sentence == tgt_sentence:\n",
        "            correct += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "    total = correct + wrong\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "\n",
        "    print(f\"the number of correct pred is: {correct}\")\n",
        "    print(f\"the number of wrong is: {wrong}\")\n",
        "    print(f\"accuracy: {acc:.4f}\")\n",
        "\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "6Vmy_-2lgrp0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model training loop"
      ],
      "metadata": {
        "id": "h4TLaKJhWgBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "IDSSKCseWjPl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbGDQJ1JWst0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attention visualization functions"
      ],
      "metadata": {
        "id": "40Av3P1EdrPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions, cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # set up ticks correctly\n",
        "    ax.set_xticks(range(len(input_sentence.split(' ')) + 1))\n",
        "    ax.set_yticks(range(len(output_words) + 1))\n",
        "\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
      ],
      "metadata": {
        "id": "LhPmh4UEdx5q"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.a. Main"
      ],
      "metadata": {
        "id": "OBkD_Ml8b7GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "#train(train_dataloader, encoder, decoder, 50, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "M0LHK83Sb-uc",
        "outputId": "980117db-10dd-4c0f-c57a-ef64bb12c8b5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 136845 sentence pairs\n",
            "Trimmed to 1570 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 1961\n",
            "eng 1199\n",
            "0m 5s (- 0m 50s) (5 10%) 2.3979\n",
            "0m 12s (- 0m 48s) (10 20%) 1.4584\n",
            "0m 17s (- 0m 41s) (15 30%) 1.0455\n",
            "0m 24s (- 0m 36s) (20 40%) 0.7748\n",
            "0m 30s (- 0m 30s) (25 50%) 0.5605\n",
            "0m 36s (- 0m 24s) (30 60%) 0.4017\n",
            "0m 42s (- 0m 18s) (35 70%) 0.2737\n",
            "0m 48s (- 0m 12s) (40 80%) 0.1806\n",
            "0m 55s (- 0m 6s) (45 90%) 0.1171\n",
            "1m 1s (- 0m 0s) (50 100%) 0.0754\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR+tJREFUeJzt3XlclOXeP/DPzAAzgMwIIvsouOWCAoIi4tJCeqzokHayFTNb7IBp1HmSTunxtNBmPztqmmadykxzzdQ0w1xQzAAxV1xQWWRYVGZggAFm5vcHMsYRlEGGe4b5vF+v+1XeXNfMdx7O43y6r01kNBqNICIiIhKIWOgCiIiIyL4xjBAREZGgGEaIiIhIUAwjREREJCiGESIiIhIUwwgREREJimGEiIiIBMUwQkRERIJyELqA1jAYDLh06RLc3NwgEomELoeIiIhawWg0oqKiAn5+fhCLW37+YRNh5NKlS1AqlUKXQURERG2Qn5+PgICAFn9uE2HEzc0NQMOHkcvlAldDREREraHRaKBUKk3f4y2xiTDSODQjl8sZRoiIiGzMraZYcAIrERERCYphhIiIiATFMEJERESCYhghIiIiQTGMEBERkaAYRoiIiEhQDCNEREQkKIYRIiIiEhTDCBEREQnKrDCSkpKCYcOGwc3NDV5eXoiLi0NOTk6r+69evRoikQhxcXHm1klERESdlFlhZM+ePUhISMDBgwexc+dO1NXVYdy4cdBqtbfse+HCBbz66qsYPXp0m4slIiKizsess2m2b9/e5M///e9/4eXlhczMTIwZM6bFfnq9Hk888QTmzZuHffv2oby8vE3FEhERUedzW3NG1Go1AMDDw+Om7f7973/Dy8sL06ZNa9Xr6nQ6aDSaJpclpJ4sxvRvMnG+7NZPdoiIiMgy2hxGDAYDZs2ahejoaAQHB7fYLi0tDStWrMDy5ctb/dopKSlQKBSmS6lUtrXMm/rm4EVsP67CxqwCi7w+ERER3Vqbw0hCQgKOHTuG1atXt9imoqICTz31FJYvXw5PT89Wv3ZycjLUarXpys/Pb2uZNzVxaAAAYMPhQhgMRou8BxEREd2cWXNGGiUmJmLLli3Yu3cvAgICWmx37tw5XLhwAbGxsaZ7BoOh4Y0dHJCTk4PevXvf0E8qlUIqlbalNLOMG+gNN6kDCq5W49CFKxjRq5vF35OIiIiaMuvJiNFoRGJiIjZu3Ihdu3YhKCjopu379++Po0ePIjs723Q9+OCDuOuuu5CdnW2x4ZfWkjlKcP8QXwDABg7VEBERCcKsJyMJCQlYtWoVfvjhB7i5uUGlUgEAFAoFnJ2dAQDx8fHw9/dHSkoKZDLZDfNJunbtCgA3nWfSkSYODcDq3/Ox7agK8x4MhrOTROiSiIiI7IpZT0aWLFkCtVqNO++8E76+vqZrzZo1pjZ5eXkoKipq90ItZVigO5QezqjU1ePnEyqhyyEiIrI7IqPRaPUzNzUaDRQKBdRqNeRyebu//v/beRqfpJ7BmH7d8fUzw9v99YmIiOxRa7+/eTYNgIlD/QEAaWdKUaypEbgaIiIi+8IwAqBnN1cMC3SHwQhsOlwodDlERER2hWHkmsY9R9ZnFcAGRq6IiIg6DYaRa+4b7AsnBzFOF1fi+CXLbD9PREREN2IYuUbh7IhxA70BNDwdISIioo7BMPInk64N1WzOvoQ6vUHgaoiIiOwDw8ifjO7rCc8uUlzW1mJPTqnQ5RAREdkFhpE/cZCIERfqBwDYcJhDNURERB2BYeR/NK6q+eVECcqragWuhoiIqPNjGPkfA/3kGOArR63egC1/2M629kRERLaKYaQZk67tyMqTfImIiCyPYaQZD4b6QSIWISuvHLmllUKXQ0RE1KkxjDTDy02GMX09AQAbuT08ERGRRTGMtKBxIuuGrEIYDNwenoiIyFIYRlpw70BvuMkcUFhejd/OXxG6HCIiok6LYaQFMkcJHhjiC4ATWYmIiCyJYeQmGodqth0tQlVtvcDVEBERdU4MIzcR0dMdPTxcoK3V4+fjxUKXQ0RE1CmZFUZSUlIwbNgwuLm5wcvLC3FxccjJyblpn+XLl2P06NFwd3eHu7s7YmJicOjQodsquqOIRCJMvLbnCE/yJSIisgyzwsiePXuQkJCAgwcPYufOnairq8O4ceOg1Wpb7LN792489thj+PXXX5Geng6lUolx48ahsNA2lsxODGsYqtl/tgwqdY3A1RAREXU+IqPR2OZ1q6WlpfDy8sKePXswZsyYVvXR6/Vwd3fHokWLEB8f36o+Go0GCoUCarUacrm8reW22SNL03HowhXMntAf08f27vD3JyIiskWt/f6+rTkjarUaAODh4dHqPlVVVairq7tpH51OB41G0+QSkmmoJrMAt5HdiIiIqBltDiMGgwGzZs1CdHQ0goODW93vtddeg5+fH2JiYlpsk5KSAoVCYbqUSmVby2wX9w3xhdRBjDMllThWKGwwIiIi6mzaHEYSEhJw7NgxrF69utV93nvvPaxevRobN26ETCZrsV1ycjLUarXpys/Pb2uZ7UIuc8S4QT4AOJGViIiovbUpjCQmJmLLli349ddfERAQ0Ko+H330Ed577z38/PPPGDJkyE3bSqVSyOXyJpfQGodqNh+5hNp6g8DVEBERdR5mhRGj0YjExERs3LgRu3btQlBQUKv6ffDBB3jrrbewfft2REREtKlQoY3u44nublJc0dZiz+lSocshIiLqNMwKIwkJCVi5ciVWrVoFNzc3qFQqqFQqVFdXm9rEx8cjOTnZ9Of3338fb775Jr744gsEBgaa+lRWVrbfp+gADhIx4kL9AHB7eCIiovZkVhhZsmQJ1Go17rzzTvj6+pquNWvWmNrk5eWhqKioSZ/a2lo8/PDDTfp89NFH7fcpOkjj9vCpJ0tQXlUrcDVERESdg4M5jVuzrHX37t1N/nzhwgVz3sKqDfCVY6CvHCeKNPjxjyI8NaKn0CURERHZPJ5NY6Y/7zlCREREt49hxEx/DfWHRCxCdn45zpXa1rwXIiIia8QwYqbublKM7dcdALAxyzbO1yEiIrJmDCNt0DhUs/FwIQwGbg9PRER0OxhG2iBmgDfcZA4oLK/GwfOXhS6HiIjIpjGMtIHMUYIHhjTsObI+k0M1REREt4NhpI0mXRuq+elYEapq6wWuhoiIyHYxjLRReE939OzmgqpaPXYcVwldDhERkc1iGGkjkUiEiWENO7JyqIaIiKjtGEZuQ+Oqmv3nylCkrr5FayIiImoOw8htUHq4YHiQB4xGYNPhS0KXQ0REZJMYRm5T40TW9VkFrTq7h4iIiJpiGLlN9w32hdRBjLMllThaqBa6HCIiIpvDMHKb3GSOGD/IBwAPzyMiImoLhpF20DiRdfORS6itNwhcDRERkW1hGGkHo/p4wstNiqtVddidUyJ0OURERDaFYaQdOEjEiAu7PpGViIiIWo9hpJ00DtXsOlWCq9pagashIiKyHWaFkZSUFAwbNgxubm7w8vJCXFwccnJybtlv7dq16N+/P2QyGQYPHoxt27a1uWBr1d9HjkF+ctTpjdjyB/ccISIiai2zwsiePXuQkJCAgwcPYufOnairq8O4ceOg1Wpb7HPgwAE89thjmDZtGg4fPoy4uDjExcXh2LFjt128tZk4tGF7+HVZ3B6eiIiotUTG29ipq7S0FF5eXtizZw/GjBnTbJvJkydDq9Viy5YtpnsjRoxAaGgoli5d2qr30Wg0UCgUUKvVkMvlbS3X4kordBiRkgq9wYhfksaij1cXoUsiIiISTGu/v29rzoha3bDJl4eHR4tt0tPTERMT0+Te+PHjkZ6e3mIfnU4HjUbT5LIF3d2kuLNfdwDABk5kJSIiapU2hxGDwYBZs2YhOjoawcHBLbZTqVTw9vZucs/b2xsqlarFPikpKVAoFKZLqVS2tcwO1zhUs/FwIQwGbg9PRER0K20OIwkJCTh27BhWr17dnvUAAJKTk6FWq01Xfn5+u7+HpdwzwAtymQOK1DU4mHtZ6HKIiIisXpvCSGJiIrZs2YJff/0VAQEBN23r4+OD4uLiJveKi4vh4+PTYh+pVAq5XN7kshUyRwkeCPEDAKzjUA0REdEtmRVGjEYjEhMTsXHjRuzatQtBQUG37BMVFYXU1NQm93bu3ImoqCjzKrUhjSf5bj+mglZXL3A1RERE1s2sMJKQkICVK1di1apVcHNzg0qlgkqlQnV1talNfHw8kpOTTX+eOXMmtm/fjvnz5+PUqVP417/+hYyMDCQmJrbfp7AyQ3u4I7CbC6pq9dhxvOW5MURERGRmGFmyZAnUajXuvPNO+Pr6mq41a9aY2uTl5aGoqMj055EjR2LVqlVYtmwZQkJCsG7dOmzatOmmk15tnUgkMk1k5fbwREREN3db+4x0FFvZZ+TP8q9UYfQHv0IkAva/djf8ujoLXRIREVGH6pB9RqhlSg8XRAZ5wGhsWOZLREREzWMYsaBJ14ZqNmQVwAYeQBEREQmCYcSCJgz2gcxRjHOlWvxRoBa6HCIiIqvEMGJBbjJHjB/UsJ8KJ7ISERE1j2HEwhpX1Ww+cgm19QaBqyEiIrI+DCMWNqqPJ7zcpCivqsOuUyVCl0NERGR1GEYsTCIW4aGwhh1ZeZIvERHRjRhGOkDjUM2vOSW4oq0VuBoiIiLrwjDSAe7wcUOwvxx1eiN+PHJJ6HKIiIisCsNIB5kYdn3PESIiIrqOYaSDPBjqBwexCEcK1DhbUiF0OURERFaDYaSDeHaR4s47ugMA1mdxe3giIqJGDCMdqHEi66bDhdAbuD08ERERwDDSoe4Z4AW5zAFF6hqkn7ssdDlERERWgWGkA0kdJIgN8QPAiaxERESNGEY6WONQzU/HVNDq6gWuhoiISHgMIx1saI+uCPJ0RXWdHj8dUwldDhERkeAYRjqYSCTCRG4PT0REZGJ2GNm7dy9iY2Ph5+cHkUiETZs23bLPt99+i5CQELi4uMDX1xfPPPMMLl+23wmccdfCSHruZRSWVwtcDRERkbDMDiNarRYhISFYvHhxq9rv378f8fHxmDZtGo4fP461a9fi0KFDeO6558wutrNQerhgRC8PGI0Ny3yJiIjsmYO5HSZMmIAJEya0un16ejoCAwPx0ksvAQCCgoLwwgsv4P333zf3rTuViUMDcDD3CtZnFeDvd/aGSCQSuiQiIiJBWHzOSFRUFPLz87Ft2zYYjUYUFxdj3bp1uO+++1rso9PpoNFomlydzX2DfSFzFCO3VIvs/HKhyyEiIhKMxcNIdHQ0vv32W0yePBlOTk7w8fGBQqG46TBPSkoKFAqF6VIqlZYus8N1kTrgL4N8AAAbuD08ERHZMYuHkRMnTmDmzJmYM2cOMjMzsX37dly4cAHTp09vsU9ycjLUarXpys/Pt3SZgmjcc+THPy5BV68XuBoiIiJhmD1nxFwpKSmIjo7GP/7xDwDAkCFD4OrqitGjR+Ptt9+Gr6/vDX2kUimkUqmlSxNcdB9PeMulKNbo8OupEvwl+Mb/WxAREXV2Fn8yUlVVBbG46dtIJBIAgNFo34fFScQi0zJfnuRLRET2yuwwUllZiezsbGRnZwMAzp8/j+zsbOTl5QFoGGKJj483tY+NjcWGDRuwZMkS5ObmYv/+/XjppZcwfPhw+Pn5tc+nsGGTrg3V/HqqBFe0tQJXQ0RE1PHMDiMZGRkICwtDWFgYACApKQlhYWGYM2cOAKCoqMgUTADg6aefxscff4xFixYhODgYf/vb33DHHXdgw4YN7fQRbFs/bzcM9leg3mDE5mw+HSEiIvsjMtrAWIlGo4FCoYBarYZcLhe6nHb35f7zmPfjCQwJUGBz4iihyyEiImoXrf3+5tk0VuDBED84iEX4o0CNM8UVQpdDRETUoRhGrEC3LlLceYcXAE5kJSIi+8MwYiUmDW1YVbPpcCH0BqsfOSMiImo3DCNW4u4BXlA4O0KlqcGBc2VCl0NERNRhGEashNRBgtiQhk3PuD08ERHZE4YRK9K4Pfz2YypU6uoFroaIiKhjMIxYkTBlV/TydEV1nR4/HS0SuhwiIqIOwTBiRUQiESZem8jKoRoiIrIXDCNW5qFrQzXpuZdRcLVK4GqIiIgsj2HEyvh3dUZUr24AGpb5EhERdXYMI1boz0M1NrBbPxER0W1hGLFCEwb7wtlRgtwyLQ7nlwtdDhERkUUxjFihLlIH/CXYBwCwIatA4GqIiIgsi2HESjUO1fx4pAi6er3A1RAREVkOw4iVGtnbEz5yGdTVddh1skTocoiIiCyGYcRKScQixIU1PB3hSb5ERNSZMYxYscaTfHfnlOBypU7gaoiIiCyDYcSK9fV2w5AABeoNRmw+cknocoiIiCzC7DCyd+9exMbGws/PDyKRCJs2bbplH51Oh3/+85/o2bMnpFIpAgMD8cUXX7SlXrszMYzbwxMRUedmdhjRarUICQnB4sWLW93nkUceQWpqKlasWIGcnBx89913uOOOO8x9a7v0YKg/HMQiHC1U43RxhdDlEBERtTsHcztMmDABEyZMaHX77du3Y8+ePcjNzYWHhwcAIDAw0Ny3tVserk64q78Xdp4oxvqsAiRPGCB0SURERO3K4nNGNm/ejIiICHzwwQfw9/dHv3798Oqrr6K6urrFPjqdDhqNpsllzxonsm46XAi9gdvDExFR52L2kxFz5ebmIi0tDTKZDBs3bkRZWRn+/ve/4/Lly/jyyy+b7ZOSkoJ58+ZZujSbcVd/L3R1cUSxRof9Z8swpl93oUsiIiJqNxZ/MmIwGCASifDtt99i+PDhuO+++/Dxxx/jq6++avHpSHJyMtRqtenKz8+3dJlWTeogQewQPwDcHp6IiDofi4cRX19f+Pv7Q6FQmO4NGDAARqMRBQXNf7FKpVLI5fIml71r3B5++3EVKmrqBK6GiIio/Vg8jERHR+PSpUuorKw03Tt9+jTEYjECAgIs/fadRqiyK3p1d0VNnQE/HVMJXQ4REVG7MTuMVFZWIjs7G9nZ2QCA8+fPIzs7G3l5eQAahlji4+NN7R9//HF069YNU6dOxYkTJ7B371784x//wDPPPANnZ+f2+RR2QCQSYdLQhvDGoRoiIupMzA4jGRkZCAsLQ1hYGAAgKSkJYWFhmDNnDgCgqKjIFEwAoEuXLti5cyfKy8sRERGBJ554ArGxsfjPf/7TTh/BfsSF+UMkAg7mXkH+lSqhyyEiImoXIqPRaPVrRTUaDRQKBdRqtd3PH3l8+UEcOHcZr9zbDzPu6St0OURERC1q7fc3z6axMRMbh2oOF8IGciQREdEtMYzYmAnBPnB2lOB8mRZZeeVCl0NERHTbGEZsjKvUAROCfQBwIisREXUODCM2qHGo5scjl1BTpxe4GiIiotvDMGKDonp3g69CBk1NPXadKhG6HCIiotvCMGKDJGIR4sIadmTlUA0REdk6hhEb1XiS7+6cUpRV6gSuhoiIqO0YRmxUHy83hAQoUG8wYnP2JaHLISIiajOGERvWOJF1PYdqiIjIhjGM2LDYED84SkQ4fkmDUyqN0OUQERG1CcOIDfNwdcJdd3gBADZmFQpcDRERUdswjNi4xqGajYcLUa83CFwNERGR+RhGbNzd/b3Q1cURJRU67DtbJnQ5REREZmMYsXFODmLEhTYs831z0zFc5jJfIiKyMQwjncDMe/qiZzcXFFytxgvfZEJXzy3iiYjIdjCMdALurk5YMSUCbjIHZFy8iuQNR2E0GoUui4iIqFUYRjqJPl5uWPz4UEjEImzIKsTSPblCl0RERNQqDCOdyJh+3TE3diAA4IMdp7DjuErgioiIiG7N7DCyd+9exMbGws/PDyKRCJs2bWp13/3798PBwQGhoaHmvi21UnxUIOKjesJoBGatzsaxQrXQJREREd2U2WFEq9UiJCQEixcvNqtfeXk54uPjcc8995j7lmSmOQ8MxOi+nqiu0+O5rzNQoqkRuiQiIqIWmR1GJkyYgLfffhsPPfSQWf2mT5+Oxx9/HFFRUea+JZnJQSLGoseHond3VxSpa/DcN5moqeMKGyIisk4dMmfkyy+/RG5uLubOnduq9jqdDhqNpslF5lE4O2LFlGHo6uKII/nleHXtEa6wISIiq2TxMHLmzBnMnj0bK1euhIODQ6v6pKSkQKFQmC6lUmnhKjunQE9XLHkiHA5iEbb8UYQFv5wRuiQiIqIbWDSM6PV6PP7445g3bx769evX6n7JyclQq9WmKz8/34JVdm5RvbvhnYeCAQCfpJ7B5iOXBK6IiIioqdY9qmijiooKZGRk4PDhw0hMTAQAGAwGGI1GODg44Oeff8bdd999Qz+pVAqpVGrJ0uzK5GE9cLakEsv3ncc/1h5BDw8XhCq7Cl0WERERAAuHEblcjqNHjza59+mnn2LXrl1Yt24dgoKCLPn29CezJwxAbqkWqadK8OxXGdicGA2/rs5Cl0VERGR+GKmsrMTZs2dNfz5//jyys7Ph4eGBHj16IDk5GYWFhfj6668hFosRHBzcpL+XlxdkMtkN98myJGIRPnksDA8vOYBTqgpM+yoD66ZHwVVq0TxKRER0S2bPGcnIyEBYWBjCwsIAAElJSQgLC8OcOXMAAEVFRcjLy2vfKqlddJE64PMpEfDs4oSTRRq8vCYbBgNX2BARkbBERhtY76nRaKBQKKBWqyGXy4Uux+ZlXryKx5YdRK3egOlje2P2hP5Cl0RERJ1Qa7+/eTaNHQrv6Y4PHh4CAFi65xzWZRYIXBEREdkzhhE7FRfmj8S7+gAAkjf8gd8vXBG4IiIislcMI3Ys6d5+mBDsgzq9ES98k4m8y1VCl0RERHaIYcSOicUizH8kBMH+clzR1mLaV79DU1MndFlERGRnGEbsnIuTAz6PHwZvuRRnSioxY9Vh1OsNQpdFRER2hGGE4KOQ4fP4YZA5irHndCne2XZS6JKIiMiOMIwQAGBwgAIfPxIKAPhy/wV8+9tFYQsiIiK7wTBCJvcN9sUr9zYcaDjnh+M4cLZM4IqIiMgeMIxQE4l390FcqB/0BiOmr8xEbmml0CUREVEnxzBCTYhEIrw3aQjCenSFpqYe077KQHlVrdBlERFRJ8YwQjeQOUqw7KkI+Hd1xvkyLf7+bRbquMKGiIgshGGEmtXdTYrPp0TA1UmCA+cuY+7m47CBY4yIiMgGMYxQiwb4yvHJo2EQiYBVv+Xhy/0XhC6JiIg6IYYRuqmYgd5Ivnaq79tbT+DXnBKBKyIios6GYYRu6bnRvfBIRAAMRmDGqsM4XVwhdElERNSJMIzQLYlEIrwdNxiRQR6o1NVj2le/43KlTuiyiIiok2AYoVZxchBj6ZPh6NnNBflXqvHCN5nQ1euFLouIiDoBhhFqNXdXJ6yYEgE3mQMyLl5F8oajXGFDRES3zewwsnfvXsTGxsLPzw8ikQibNm26afsNGzbg3nvvRffu3SGXyxEVFYUdO3a0tV4SWB8vNyx+fCgkYhE2ZBVi6Z5coUsiIiIbZ3YY0Wq1CAkJweLFi1vVfu/evbj33nuxbds2ZGZm4q677kJsbCwOHz5sdrFkHcb06465sQMBAB/sOIUdx1UCV0RERLZMZLyN5+wikQgbN25EXFycWf0GDRqEyZMnY86cOa1qr9FooFAooFarIZfL21ApWcKbm47hm4MX4ewowdrpUQj2VwhdEhERWZHWfn93+JwRg8GAiooKeHh4tNhGp9NBo9E0ucj6zI0diNF9PVFdp8dzX2egRFMjdElERGSDOjyMfPTRR6isrMQjjzzSYpuUlBQoFArTpVQqO7BCai0HiRiLHh+KXt1dUaSuwXPfZKKmjitsiIjIPB0aRlatWoV58+bh+++/h5eXV4vtkpOToVarTVd+fn4HVknmUDg74ospw9DVxRFH8svx6tojXGFDRERm6bAwsnr1ajz77LP4/vvvERMTc9O2UqkUcrm8yUXWK9DTFUueCIeDWIQtfxThk9QzQpdEREQ2pEPCyHfffYepU6fiu+++w/33398Rb0kdLKp3N7zzUDAAYMEvZ/DjkUsCV0RERLbC7DBSWVmJ7OxsZGdnAwDOnz+P7Oxs5OXlAWgYYomPjze1X7VqFeLj4zF//nxERkZCpVJBpVJBrVa3zycgqzF5WA88NzoIAPDq2iPIzi8XtiAiIrIJZoeRjIwMhIWFISwsDACQlJSEsLAw0zLdoqIiUzABgGXLlqG+vh4JCQnw9fU1XTNnzmynj0DWZPaEAbinvxd09QY893UGLpVXC10SERFZudvaZ6SjcJ8R21Kpq8fDSw7glKoCA33lWDs9Cq5SB6HLIiKiDma1+4xQ59dF6oDPp0TAs4sTThRp8PKabBgMVp95iYhIIAwjZBEB7i747KlwOEnE+PlEMT78OUfokoiIyEoxjJDFhPf0wAcPDwEALNl9DusyCwSuiIiIrBHDCFlUXJg/Eu/qAwBI3vAHfr9wReCKiIjI2jCMkMUl3dsPE4J9UKc34oVvMpF3uUrokoiIyIowjJDFicUizH8kBMH+clzR1mLaV7+joqZO6LKIiMhKMIxQh3BxcsDn8cPgLZfiTEklZnx3GPV6g9BlERGRFWAYoQ7jo5BheXwEZI5i7M4pxTvbTgpdEhERWQGGEepQQwK64uNHQgEAX+6/gG9/uyhsQUREJDiGEepw9w32xSv39gMAzP3hOA6cLRO4IiIiEhLDCAki8e4++GuoH+oNRkxfmYnc0kqhSyIiIoEwjJAgRCIR3p80BGE9ukJTU49pX2VAXcUVNkRE9ohhhAQjc5Rg2VMR8O/qjPNlWrz4bSbquMKGiMjuMIyQoLq7SfH5lAi4Oklw4NxlzN18HDZwkDQREbUjhhES3ABfOT55NAwiEbDqtzzM//k0T/klIrIjDCNkFWIGeuP1CQMAAIt+PYtnv+YcEiIie8EwQlbjuTG98OHDQyB1EGPXqRLELkrD8UtqocsiIiILYxghq/K3CCXWvzgSAe7OyLtShYmfHsD6zAKhyyIiIgsyO4zs3bsXsbGx8PPzg0gkwqZNm27ZZ/fu3Rg6dCikUin69OmD//73v20olexFsL8CW2aMwth+3aGrN+CVtUfw5qZjqK3nShsios7I7DCi1WoREhKCxYsXt6r9+fPncf/99+Ouu+5CdnY2Zs2ahWeffRY7duwwu1iyH11dnPDl08Mw856+AIBvDl7E5GXpKFJXC1wZERG1N5HxNtZRikQibNy4EXFxcS22ee2117B161YcO3bMdO/RRx9FeXk5tm/f3qr30Wg0UCgUUKvVkMvlbS2XbNSuU8WYtTobmpp6dHN1wsLHwzCyt6fQZRER0S209vvb4nNG0tPTERMT0+Te+PHjkZ6e3mIfnU4HjUbT5CL7dXd/b2yZMRoDfOW4rK3Fk5//hs/2nON+JEREnYTFw4hKpYK3t3eTe97e3tBoNKiubv6Re0pKChQKhelSKpWWLpOsXI9uLtjw4khMHOoPgxFI+ekUXlyZhYoaLv8lIrJ1VrmaJjk5GWq12nTl5+cLXRJZAWcnCeb/LQRvxQXDUSLC9uMqxC3ej7MlFUKXRkREt8HiYcTHxwfFxcVN7hUXF0Mul8PZ2bnZPlKpFHK5vMlFBDTMU3pqRE+seSEKPnIZzpVq8ddF+7H1jyKhSyMiojayeBiJiopCampqk3s7d+5EVFSUpd+aOrGhPdyx5aVRiOrVDdpaPRJWZeGdrSdQz4P2iIhsjtlhpLKyEtnZ2cjOzgbQsHQ3OzsbeXl5ABqGWOLj403tp0+fjtzcXPzf//0fTp06hU8//RTff/89Xn755fb5BGS3PLtI8c204XhhbC8AwPJ95/HE57+htEIncGVERGQOs8NIRkYGwsLCEBYWBgBISkpCWFgY5syZAwAoKioyBRMACAoKwtatW7Fz506EhIRg/vz5+PzzzzF+/Ph2+ghkzxwkYiRPGIAlTwyFq5MEv52/ggcW7kPmxStCl0ZERK10W/uMdBTuM0KtcbakEtNXZuJsSSUcJSK8cf9AxEf1hEgkEro0IiK7ZDX7jBB1lD5eXbApIRr3D/ZFnd6IuZuPI+n7I6iu1QtdGhER3QTDCHUqXaQOWPR4GN64fwAkYhE2Hi7EQ5/ux4UyrdClERFRCxhGqNMRiUR4dnQvfPtsJDy7OOGUqgKxi9Lwy4niW3cmIqIOxzBCndaIXt2wZcZoDO3RFRU19Xj26wzM/zkHeoPVT5MiIrIrDCPUqfkoZFj9fBSeHhkIAFi46yym/vd3XNXWClsYERGZMIxQp+fkIMa/HhyE/zc5BDJHMfaeLsUDC9NwtEAtdGlERASGEbIjD4UFYOPfo9GzmwsKy6sxaekBfP87zz0iIhIawwjZlQG+cmxOHIWYAV6orTfg/9b/geQNf6Cmjst/iYiEwjBCdkfh7IhlT0Xg1XH9IBIB3x3KxyOfpaPgapXQpRER2SWGEbJLYrEIiXf3xVdTh6OriyP+KFAjdmEa9p0pFbo0IiK7wzBCdm1Mv+74MXEUBvsrcLWqDlO+OITFv56Fgct/iYg6DMMI2T2lhwvWTo/C5AglDEbgwx05eGFlJjQ1dUKXRkRkFxhGiADIHCV4/+EheG/iYDhJxNh5ohgPLkxDjqpC6NKIiDo9hhGiP3l0eA+snR4F/67OuHC5CnGL9+OH7EKhyyIi6tQYRoj+R4iyK36cMQqj+3qiuk6Pmauz8a/Nx1GnNwhdGhFRp8QwQtQMD1cn/HfqcCTe1QcA8N8DF/DYsoMo0dQIXBkRUefDMELUAolYhFfH34Hl8RFwkzog4+JV3L8wDYfOXxG6NCKiTqVNYWTx4sUIDAyETCZDZGQkDh06dNP2CxYswB133AFnZ2colUq8/PLLqKnhf2GSbbh3oDc2zxiFO7zdUFqhw2PLD2JF2nkYjVz+S0TUHswOI2vWrEFSUhLmzp2LrKwshISEYPz48SgpKWm2/apVqzB79mzMnTsXJ0+exIoVK7BmzRq8/vrrt108UUcJ8nTFxoSR+GuoH/QGI97acgIzvjsMra5e6NKIiGyeyGjmf95FRkZi2LBhWLRoEQDAYDBAqVRixowZmD179g3tExMTcfLkSaSmppruvfLKK/jtt9+QlpbWqvfUaDRQKBRQq9WQy+XmlEvUroxGI746cAFvbz2JeoMR/by7YOmT4ejVvYvQpRERWZ3Wfn+b9WSktrYWmZmZiImJuf4CYjFiYmKQnp7ebJ+RI0ciMzPTNJSTm5uLbdu24b777jPnrYmsgkgkwtPRQVj9/Ah4uUlxurgSDy7aj+3HVEKXRkRks8wKI2VlZdDr9fD29m5y39vbGypV838ZP/744/j3v/+NUaNGwdHREb1798add95502EanU4HjUbT5CKyJhGBHtjy0igMD/JApa4e01dm4v3tp1DP5b9ERGaz+Gqa3bt3491338Wnn36KrKwsbNiwAVu3bsVbb73VYp+UlBQoFArTpVQqLV0mkdm83GT49tlIPDsqCACwZPc5TPnyEC5X6gSujIjItpg1Z6S2thYuLi5Yt24d4uLiTPenTJmC8vJy/PDDDzf0GT16NEaMGIEPP/zQdG/lypV4/vnnUVlZCbH4xjyk0+mg013/C12j0UCpVHLOCFmtH49cwmvr/0BVrR6+ChmWPBmOUGVXocsiIhKUReaMODk5ITw8vMlkVIPBgNTUVERFRTXbp6qq6obAIZFIAKDFpZFSqRRyubzJRWTNYkP8sCkhGr08XVGkrsEjS9Px+b5cDtsQEbWC2cM0SUlJWL58Ob766iucPHkSL774IrRaLaZOnQoAiI+PR3Jysql9bGwslixZgtWrV+P8+fPYuXMn3nzzTcTGxppCCVFn0M/bDT8kRmP8IG/U6g14e+tJ/HXxfmTnlwtdGhGRVXMwt8PkyZNRWlqKOXPmQKVSITQ0FNu3bzdNas3Ly2vyJOSNN96ASCTCG2+8gcLCQnTv3h2xsbF455132u9TEFkJN5kjlj4Zju8O5eO9n07i+CUNHvp0P54a0ROvjr8Dcpmj0CUSEVkds/cZEQL3GSFbVFapw7tbT2LD4YZTf7u7SfHmAwMRO8QXIpFI4OqIiCzPInNGiKj1PLtI8fHkUKx6NhK9PF1RWqHDS98dRvwXh3ChTCt0eUREVoNhhMjCRvbxxE+zRuPlmH5wchBj35kyjFuwFwtTz0BXrxe6PCIiwTGMEHUAqYMEM2P6YsesMRjVxxO19QbM33kaEz7ZhwPnyoQuj4hIUAwjRB0oyNMV30wbjk8eDYVnFylyS7V4fPlvSPo+G2XcLI2I7BTDCFEHE4lE+GuoP1JfGYunRvSESARsyCrEPfP34LtDeTAYrH5OORFRu+JqGiKBHc67in9uPIYTRQ1nMIX3dMc7DwWjvw//t05Eto2raYhsRFgPd2xOjMYb9w+Ai5MEmRev4oH/pCHlp5Ooqq0XujwiIotjGCGyAg4SMZ4d3Qu/JI3FXwb5oN5gxGd7cnHvx3vxy4liocsjIrIohhEiK+LX1RlLnwrHiikR8O/qjMLyajz7dQZe+CYDl8qrhS6PiMgiGEaIrNA9A7yxM2kMpo/tDQexCDuOF+Pej/fw8D0i6pQYRoislIuTA2ZP6I8tL41CeE93aGv1eHvrScQu2o/DeVeFLo+IqN0wjBBZuf4+cqx9IQrvTRwMhbMjThZpMHHJAbyx6SjU1XVCl0dEdNsYRohsgFgswqPDe2DXK2MxaWgAjEZg5cE83DN/D37ILoQNrNAnImoRwwiRDenWRYr5j4Tgu+dGoFd3V5RV6jBzdTbivziE8zx8j4hsFMMIkQ2K6t0NP80cjVfH9YP02uF74xfsxSe/8PA9IrI9DCNENkrqIEHi3X3x88tjMLpvw+F7/++X05iwYB8OnOXhe0RkOxhGiGxcz26u+PqZ4Vj4WBi6u0mRW6bF45//hpfX8PA9IrINDCNEnYBIJEJsiB9SXxmLKVENh+9tPFyIuz/ajVW/8fA9IrJubQojixcvRmBgIGQyGSIjI3Ho0KGbti8vL0dCQgJ8fX0hlUrRr18/bNu2rU0FE1HL5DJHzPtrMDb9PRqD/OTQ1NTj9Y1H8fDSAzh57SA+IiJrY3YYWbNmDZKSkjB37lxkZWUhJCQE48ePR0lJSbPta2trce+99+LChQtYt24dcnJysHz5cvj7+9928UTUvBBlV/yQEI25sQPRReqArLxyPLAwDe9uOwmtjofvEZF1ERnN3KAgMjISw4YNw6JFiwAABoMBSqUSM2bMwOzZs29ov3TpUnz44Yc4deoUHB0d21Rka48gJqIbqdQ1mPfjcfx0TAUA8FPIMO+vwbh3oLfAlRFRZ9fa72+znozU1tYiMzMTMTEx119ALEZMTAzS09Ob7bN582ZERUUhISEB3t7eCA4Oxrvvvgu9nssPiTqCj0KGJU+G44unIxDg7oxL6ho893UGnvs6A4U8fI+IrIBZYaSsrAx6vR7e3k3/i8rb2xsqlarZPrm5uVi3bh30ej22bduGN998E/Pnz8fbb7/d4vvodDpoNJomFxHdnrv7e2Pny2Px9zsbDt/beaLh8L3le3NRx8P3iEhAFl9NYzAY4OXlhWXLliE8PByTJ0/GP//5TyxdurTFPikpKVAoFKZLqVRaukwiu+DsJMH//aU/ts0cjWGB7qiq1eOdbScRuzANWTx8j4gEYlYY8fT0hEQiQXFxcZP7xcXF8PHxabaPr68v+vXrB4lEYro3YMAAqFQq1NbWNtsnOTkZarXadOXn55tTJhHdQj9vN6x5PgofPDwE7i6OOKWqwKQlB/D6xqNQV/HwPSLqWGaFEScnJ4SHhyM1NdV0z2AwIDU1FVFRUc32iY6OxtmzZ2EwXH8MfPr0afj6+sLJyanZPlKpFHK5vMlFRO1LLBbhkQglUl+5Ew+HNxy+t+q3PNzz8W5sOszD94io45g9TJOUlITly5fjq6++wsmTJ/Hiiy9Cq9Vi6tSpAID4+HgkJyeb2r/44ou4cuUKZs6cidOnT2Pr1q149913kZCQ0H6fgojazMPVCR/9LQSrnx+BPl5dUFZZi1lrsvHkit+QW1opdHlEZAcczO0wefJklJaWYs6cOVCpVAgNDcX27dtNk1rz8vIgFl/POEqlEjt27MDLL7+MIUOGwN/fHzNnzsRrr73Wfp+CiG7biF7dsO2l0Vi+Lxf/ST2D/Wcv4y8L9uHp6EBMGxUEb7lM6BKJqJMye58RIXCfEaKOlXe5Cm/+cAx7TpcCABwlIsSF+uP5Mb3Q19tN4OqIyFa09vubYYSImmU0GvFrTgmW7s7FoQtXTPfv6e+F58f0wvAgD4hEIgErJCJrxzBCRO0mK+8qlu3JxY4TKjT+jRGq7IoXxvTCuEE+kIgZSojoRgwjRNTucksr8XnaeazLLEBtfcMKucBuLnh2dC88HB4AmaPkFq9ARPaEYYSILKa0Qoev0y/g6/SLUFc37EvSzdUJU0YG4qkRPeHu2vyyfSKyLwwjRGRxWl09vs/Ix+f7zpvOuXF2lGDyMCWmjQqC0sNF4AqJSEgMI0TUYer1Bmw9WoRle3Nx/FLDWVJiEXD/ED+8MKYXgv0VAldIREJgGCGiDmc0GrH/7GV8tvcc9p0pM90f2bsbXhjbG2P6enIFDpEdYRghIkEdv6TG8r25+PGPIugNDX/N9Pdxw/NjeiE2xA+OEouf00lEAmMYISKrUFhejS/SzuO7Q3moqtUDAPwUMjwzKgiPDu+BLlKzN4ImIhvBMEJEVkVdVYeVv13El/svoKxSBwBwkzngyRE9MXVkILy43TxRp8MwQkRWqaZOj02HC7Fsby5yy7QAACeJGA+F+eO5Mb3Qx6uLwBUSUXthGCEiq2YwGPHLyWJ8tjcXmRevmu7HDPDG9LG9EBHoIWB1RNQeGEaIyGZkXryCz/bkYufJYtN280N7dMXzY3pj3EBviLndPJFNYhghIptzrrQSn+/LxfrMQtTqG7ab7+XpimdH98LEof7cbp7IxjCMEJHNKqmowVcHLuCb9IvQ1NQDADy7SPH0yJ54ckRPdHXhdvNEtoBhhIhsXqWuHmt+z8cXade3m3dxur7dfIA7t5snsmYMI0TUadTpDdj6RxE+25uLk0UN281LxCI8MMQXz4/phUF+3G6eyBoxjBBRp2M0GrHvTBmW7c1F2tnr282P6uOJF8b2wqg+3G6eyJq09vu7TfsxL168GIGBgZDJZIiMjMShQ4da1W/16tUQiUSIi4try9sSkZ0TiUQY0687Vj4biS0zRuHBED9IxCKknS3DUysO4f7/pOGH7ELUXZv8SkS2wewnI2vWrEF8fDyWLl2KyMhILFiwAGvXrkVOTg68vLxa7HfhwgWMGjUKvXr1goeHBzZt2tTq9+STESJqSf6VKqxIO481v+ejuq5hu3n/rs6YNioIk4cp4crt5okEY7FhmsjISAwbNgyLFi0CABgMBiiVSsyYMQOzZ89uto9er8eYMWPwzDPPYN++fSgvL2cYIaJ2dVVbi5UHL+Kr9Asoq6wFACicHfHUiJ6YMjIQ3d2kAldIZH8sMkxTW1uLzMxMxMTEXH8BsRgxMTFIT09vsd+///1veHl5Ydq0aea8HRFRq7m7OmHGPX2R9trdePehwQjydIW6ug6Lfj2L6Pd34dW1R5B+7jIMBqufJkdkd8x6fllWVga9Xg9vb+8m9729vXHq1Klm+6SlpWHFihXIzs5u9fvodDrodDrTnzUajTllEpEdkzlK8HhkD0wepsTOE8X4bO85HM4rx7rMAqzLLECAuzMmhvnjoaEBCPJ0FbpcIoKZYcRcFRUVeOqpp7B8+XJ4enq2ul9KSgrmzZtnwcqIqLOTiEX4S7APxg/yRlZeOdZm5GPrH0UouFqN/+w6i//sOovwnu6YONQfDwzxg8LZUeiSieyWWXNGamtr4eLignXr1jVZETNlyhSUl5fjhx9+aNI+OzsbYWFhkEiub+FsMDTMcheLxcjJyUHv3r1veJ/mnowolUrOGSGi21JTp8fPJ4qxPrMA+86UonHExslBjHsHeGNSuD/G9O0OB0mbFhoS0f+w6ATW4cOHY+HChQAawkWPHj2QmJh4wwTWmpoanD17tsm9N954AxUVFfjkk0/Qr18/ODndeltnTmAlovZWoqnBpuxCrM8sRE5xhem+Zxcp/hrqh0lDAzDQj3/fEN0Oi4WRNWvWYMqUKfjss88wfPhwLFiwAN9//z1OnToFb29vxMfHw9/fHykpKc32f/rpp7mahoishtFoxPFLGqzPKsDm7Eu4rK01/WyArxyThvrjr6H+XI1D1Aat/f42e87I5MmTUVpaijlz5kClUiE0NBTbt283TWrNy8uDWMxHnERkG0QiEYL9FQj2V+D1+wZgT04p1mcVIPVkCU4WafD2Vg1SfjqFsf26Y+JQf8QM8ObpwUTtjNvBExE1o7yqFj/+UYT1mQXIzi833XeTOeCBIX54ONwfQ3u4c/t5opvg2TRERO3kXGklNmQVYGNWIS6pa0z3A7u5YOLQADwU5g+lB08QJvpfDCNERO3MYDDiYO5lrMsqwPZjKlTV6k0/G9HLAxOHBuC+wb7owi3oiQAwjBARWZRWV4/tx1RYn1WA9NzLaPybVOYoxl8G+WBSeABG9vaERMxhHLJfDCNERB2ksLwamw4XYn1mAXLLtKb7PnIZ4sL88XC4P/p4uQlYIZEwGEaIiDqY0WhEdn451mcV4McjRVBX15l+FhKgwMShAXgwxA/urrfeX4moM2AYISISkK5ej10nS7A+qwC7c0pRf227V0eJCHfd4YVJ4QG46w4vODlwKwTqvBhGiIisRFmlDpuzL2F9VgGOX7p+8Ke7iyMeDPHDpPAADPZXcJkwdToMI0REVuiUSoMNWYXYeLgQpRXXz+Dq69XFtEzYRyETsEKi9sMwQkRkxer1BqSdLcP6rEL8fFwFXf21Q0RFQHQfT0waGoDxg3zg7MTdXsl2MYwQEdkITU0dtv1RhPVZBfj9wlXTfVcnCe4b7ItJ4QEYHugBMZcJk41hGCEiskEXL2uxIasQGw4XIP9Ktel+gLszJob548FQP/Tu3oXzS8gmMIwQEdkwg8GIjItXsT6zAFuPFqFSV2/6ma9ChlF9PDGqryei+3jCswtPFCbrxDBCRNRJVNfq8fMJFTZkFSI99zJqr80vaTTAV47RfT0xqo8nhgd58FRhshoMI0REnVB1rR6/X7iCtLNl2HemDCeLNE1+7uQgRkRPd4zq64nRfbpjkJ+cc01IMAwjRER2oLRChwPnypB2pgxpZ8tQ9KdThYGGvUxG9vHE6D4NQzo8XZg6EsMIEZGdMRqNOFeqRdqZUqSdvYyDuZebzDUBgMBuLhjV1xOj+nRHVO9uUDg7ClQt2QOGESIiO1enN+BIfjn2XXtqkp1fDr3h+l/5YhEQouzaMBm2jyfCerhze3pqVwwjRETUhKamDr/lXkHamVLsO1uG3FJtk5+7OEkwolc3jOrjidF9PdHHi0uI6fZYNIwsXrwYH374IVQqFUJCQrBw4UIMHz682bbLly/H119/jWPHjgEAwsPD8e6777bYvjkMI0RE7a+wvBr7rz012X+2DJe1tU1+7i2XIvpaMInu4wkvN25TT+axWBhZs2YN4uPjsXTpUkRGRmLBggVYu3YtcnJy4OXldUP7J554AtHR0Rg5ciRkMhnef/99bNy4EcePH4e/v3+7fhgiImobg8GIkyqNaSLsofNXTFvUN+rv42ba3yQyqBu3qqdbslgYiYyMxLBhw7Bo0SIAgMFggFKpxIwZMzB79uxb9tfr9XB3d8eiRYsQHx/fqvdkGCEi6lg1dXpkXrx6bb5JKY5f0uDP3xZOEjHCry0hHtXHE8H+Cki4hJj+R2u/vx3MedHa2lpkZmYiOTnZdE8sFiMmJgbp6emteo2qqirU1dXBw8OjxTY6nQ463fXTLDUaTYttiYio/ckcJYi+thwY6I8r2lrsP3t9CXFheTXScy8jPfcyPtyRg64ujhjZuxtG9emO0X25hJjMY1YYKSsrg16vh7e3d5P73t7eOHXqVKte47XXXoOfnx9iYmJabJOSkoJ58+aZUxoREVmQh6sTYkP8EBviB6PRiAuXqxomwp4pQ/q5yyivqsO2oypsO6oCAPTwcLm28ZonRvb2hMKFS4ipZWaFkdv13nvvYfXq1di9ezdkspYnQiUnJyMpKcn0Z41GA6VS2RElEhHRLYhEIgR5uiLI0xVPRQWiXm/AH4XqhqcmZ8qQlXcVeVeqsOq3PKz6LQ9iETA4oKtp47XwnlxCTE2ZFUY8PT0hkUhQXFzc5H5xcTF8fHxu2vejjz7Ce++9h19++QVDhgy5aVupVAqplAc/ERHZAgeJGEN7uGNoD3e8dE9fVOrq8VvuZew707BK50xJJY7kl+NIfjkW/XoWzo4ShPd0x0A/OQb4umGArxy9u3eBo4QBxV6ZFUacnJwQHh6O1NRUxMXFAWiYwJqamorExMQW+33wwQd45513sGPHDkRERNxWwUREZN26SB1wzwBv3DOgYUhfpa5B2tky086wZZW6hj+fLTP1cZKI0cerCwb4NgSUgb5yDPCVw93VSaiPQR3I7GGapKQkTJkyBRERERg+fDgWLFgArVaLqVOnAgDi4+Ph7++PlJQUAMD777+POXPmYNWqVQgMDIRK1TCe2KVLF3Tp0qUdPwoREVkjH4UMD4cH4OHwABiNRpxSVSA7vxwnizQ4WaTBqaIKVOjqcaJIgxP/c/Cft1x6LaA0XAN93RDk2YUrdzoZs8PI5MmTUVpaijlz5kClUiE0NBTbt283TWrNy8uDWHz9UduSJUtQW1uLhx9+uMnrzJ07F//6179ur3oiIrIpIpHIFCwaGY1GFFytvhZOKhr+qdLg4uUqFGt0KNaUYndOqam91EGMO3zcMMDn+jBPf185z9mxYdwOnoiIrFKlrh45Kg1ONAaUIg1yVBWoqtU3296/q7Pp6UljQOnp4QIxn6IIhmfTEBFRp2MwGHHxSpUpnDQ+SSksr262vYuTpOEpyp+Gee7wkaOLtEMXk9othhEiIrIb6uo6nPpzQFE1PEX53y3tG/Xs5nJtmOf6UE+AuzMPBmxnDCNERGTX6vUGXLisbTLMc7JIg2KNrtn2bjKHG+ah3OHtxjN4bgPDCBERUTOuaGtvGOY5U1KBOv2NX4diERDo6XptiOd6UPGRy/gUpRUYRoiIiFqpTm/AudLKpit6ijQoq6xttn1XF0cM8JHjDh83+Hd1hr+7s+mf3VydGFSuYRghIiK6TSUVNU3CyckiDc6VaqE3tPzVKXUQNw0o1/7d79q/+yhkdrPbrEVO7SUiIrInXm4yeLnJMLZfd9O9mjo9zpZUmoJJYXk1Cq9W4VJ5DYoraqCrNyC3TIvcMm2zrykWAT5yWZOA0hhcAq7dc3Gyr69n+/q0REREt0nmKEGwvwLB/oobflZbb4BKXYOC8ioUXq1GYXk1LpVXXwss1bhUXoNavQGX1DW4pK4BcLXZ93B3cfzTkxUX+HWVIcC94d/93Z3h7uLYqYaCGEaIiIjaiZODGD26uaBHN5dmf24wGFFWqUNBY0i5ej2oNP6zQlePq1V1uFpVh2OFmmZfx9lRAr+uMvi7u5ieqPh3vfakxd0Z3m5SONjQUBDDCBERUQcRi0XwksvgJZdhaA/3ZttoauoawkljQClvGlhKK3SortPjXKkW50qbHwqSiEWmoSD//xkKavynzNF6liwzjBAREVkRucwRcl/HJuf3/FlNnR5F6hrTk5UCU1CpQmF5NYrKa1BvMJpCTEu6uTo1CSsPhvphSEBXC32qm2MYISIisiEyRwmCPF0R5Ona7M/1BiNKK3QoLK9CwZ/nrfxpKEhbq8dlbS0ua2vxR4EaADA4QMEwQkRERLdPIhbBRyGDj0KG8J43/txoNEJdXYeCq00n1w7yu3FCbkdhGCEiIrIjIpEIXV2c0NXFqdkVQUKwnam2RERE1CkxjBAREZGgGEaIiIhIUAwjREREJKg2hZHFixcjMDAQMpkMkZGROHTo0E3br127Fv3794dMJsPgwYOxbdu2NhVLREREnY/ZYWTNmjVISkrC3LlzkZWVhZCQEIwfPx4lJSXNtj9w4AAee+wxTJs2DYcPH0ZcXBzi4uJw7Nix2y6eiIiIbJ/IaDS2fA5yMyIjIzFs2DAsWrQIAGAwGKBUKjFjxgzMnj37hvaTJ0+GVqvFli1bTPdGjBiB0NBQLF26tFXv2dojiImIiMh6tPb726wnI7W1tcjMzERMTMz1FxCLERMTg/T09Gb7pKenN2kPAOPHj2+xPQDodDpoNJomFxEREXVOZoWRsrIy6PV6eHt7N7nv7e0NlUrVbB+VSmVWewBISUmBQqEwXUql0pwyiYiIyIZY5Wqa5ORkqNVq05Wfny90SURERGQhZm0H7+npCYlEguLi4ib3i4uL4ePj02wfHx8fs9oDgFQqhVQqNac0IiIislFmPRlxcnJCeHg4UlNTTfcMBgNSU1MRFRXVbJ+oqKgm7QFg586dLbYnIiIi+2L2QXlJSUmYMmUKIiIiMHz4cCxYsABarRZTp04FAMTHx8Pf3x8pKSkAgJkzZ2Ls2LGYP38+7r//fqxevRoZGRlYtmxZ+34SIiIisklmh5HJkyejtLQUc+bMgUqlQmhoKLZv326apJqXlwex+PoDl5EjR2LVqlV444038Prrr6Nv377YtGkTgoODW/2ejauPuaqGiIjIdjR+b99qFxGz9xkRQkFBAVfUEBER2aj8/HwEBAS0+HObCCMGgwGXLl2Cm5sbRCJRu72uRqOBUqlEfn4+N1OzEvydWBf+PqwLfx/Whb+PWzMajaioqICfn1+TUZP/ZfYwjRDEYvFNE9Xtksvl/B+SleHvxLrw92Fd+PuwLvx93JxCobhlG6vcZ4SIiIjsB8MIERERCcquw4hUKsXcuXO5wZoV4e/EuvD3YV34+7Au/H20H5uYwEpERESdl10/GSEiIiLhMYwQERGRoBhGiIiISFAMI0RERCQouw4jixcvRmBgIGQyGSIjI3Ho0CGhS7JLKSkpGDZsGNzc3ODl5YW4uDjk5OQIXRZd895770EkEmHWrFlCl2LXCgsL8eSTT6Jbt25wdnbG4MGDkZGRIXRZdkmv1+PNN99EUFAQnJ2d0bt3b7z11lu3PH+FWma3YWTNmjVISkrC3LlzkZWVhZCQEIwfPx4lJSVCl2Z39uzZg4SEBBw8eBA7d+5EXV0dxo0bB61WK3Rpdu/333/HZ599hiFDhghdil27evUqoqOj4ejoiJ9++gknTpzA/Pnz4e7uLnRpdun999/HkiVLsGjRIpw8eRLvv/8+PvjgAyxcuFDo0myW3S7tjYyMxLBhw7Bo0SIADeffKJVKzJgxA7Nnzxa4OvtWWloKLy8v7NmzB2PGjBG6HLtVWVmJoUOH4tNPP8Xbb7+N0NBQLFiwQOiy7NLs2bOxf/9+7Nu3T+hSCMADDzwAb29vrFixwnRv0qRJcHZ2xsqVKwWszHbZ5ZOR2tpaZGZmIiYmxnRPLBYjJiYG6enpAlZGAKBWqwEAHh4eAldi3xISEnD//fc3+f8TEsbmzZsRERGBv/3tb/Dy8kJYWBiWL18udFl2a+TIkUhNTcXp06cBAEeOHEFaWhomTJggcGW2yyYOymtvZWVl0Ov18Pb2bnLf29sbp06dEqgqAhqeUM2aNQvR0dEIDg4Wuhy7tXr1amRlZeH3338XuhQCkJubiyVLliApKQmvv/46fv/9d7z00ktwcnLClClThC7P7syePRsajQb9+/eHRCKBXq/HO++8gyeeeELo0myWXYYRsl4JCQk4duwY0tLShC7FbuXn52PmzJnYuXMnZDKZ0OUQGkJ6REQE3n33XQBAWFgYjh07hqVLlzKMCOD777/Ht99+i1WrVmHQoEHIzs7GrFmz4Ofnx99HG9llGPH09IREIkFxcXGT+8XFxfDx8RGoKkpMTMSWLVuwd+9eBAQECF2O3crMzERJSQmGDh1quqfX67F3714sWrQIOp0OEolEwArtj6+vLwYOHNjk3oABA7B+/XqBKrJv//jHPzB79mw8+uijAIDBgwfj4sWLSElJYRhpI7ucM+Lk5ITw8HCkpqaa7hkMBqSmpiIqKkrAyuyT0WhEYmIiNm7ciF27diEoKEjokuzaPffcg6NHjyI7O9t0RURE4IknnkB2djaDiACio6NvWO5++vRp9OzZU6CK7FtVVRXE4qZfnxKJBAaDQaCKbJ9dPhkBgKSkJEyZMgUREREYPnw4FixYAK1Wi6lTpwpdmt1JSEjAqlWr8MMPP8DNzQ0qlQoAoFAo4OzsLHB19sfNze2G+Tqurq7o1q0b5/EI5OWXX8bIkSPx7rvv4pFHHsGhQ4ewbNkyLFu2TOjS7FJsbCzeeecd9OjRA4MGDcLhw4fx8ccf45lnnhG6NNtltGMLFy409ujRw+jk5GQcPny48eDBg0KXZJcANHt9+eWXQpdG14wdO9Y4c+ZMocuwaz/++KMxODjYKJVKjf379zcuW7ZM6JLslkajMc6cOdPYo0cPo0wmM/bq1cv4z3/+06jT6YQuzWbZ7T4jREREZB3scs4IERERWQ+GESIiIhIUwwgREREJimGEiIiIBMUwQkRERIJiGCEiIiJBMYwQERGRoBhGiIiISFAMI0RERCQohhEiIiISFMMIERERCYphhIiIiAT1/wHNg9+Z8/FVXAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.b. Evaluation on 20  random sentences"
      ],
      "metadata": {
        "id": "hb_GbZwveGck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder,decoder,20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gF418xVeJM4",
        "outputId": "bc5f362b-bf02-4d89-e8d1-3116b260244b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> את דו-פרצופית .\n",
            "= you are two-faced .\n",
            "< you are two-faced . <EOS>\n",
            "\n",
            "> אני משחק שחמט מצוין .\n",
            "= i am a very good chess player .\n",
            "< i am a very good chess player . <EOS>\n",
            "\n",
            "> הוא איטי מחשבה .\n",
            "= he is thick-headed .\n",
            "< he is thick-headed . <EOS>\n",
            "\n",
            "> הוא שר יפה .\n",
            "= he is good at singing .\n",
            "< he is good at singing . <EOS>\n",
            "\n",
            "> אתה יודע מה שאתה רוצה .\n",
            "= you are strong-minded .\n",
            "< you are strong-minded . <EOS>\n",
            "\n",
            "> אני טבח .\n",
            "= i am a cook .\n",
            "< i am a cook cook . <EOS>\n",
            "\n",
            "> אנו מצפים לתוספת למשפחה שלנו .\n",
            "= we are expecting an addition to our family .\n",
            "< we are expecting an addition to our family . <EOS>\n",
            "\n",
            "> הוא מפחד לשחות .\n",
            "= he is afraid of swimming .\n",
            "< he is afraid of swimming in swimming . <EOS>\n",
            "\n",
            "> היא עומדת לעזוב .\n",
            "= she is about to leave .\n",
            "< she is about to leave this building . <EOS>\n",
            "\n",
            "> היא אינה רק ישרה אלא גם נבונה .\n",
            "= she is not only honest, but also wise .\n",
            "< she is not only honest, but also wise . <EOS>\n",
            "\n",
            "> הם מרוצים מהחוזה .\n",
            "= they are satisfied with the contract .\n",
            "< i am going to the laws of the best friends\n",
            "\n",
            "> הביקורת שלו צורמת .\n",
            "= he is a harsh critic .\n",
            "< he is a harsh critic . <EOS>\n",
            "\n",
            "> אני מוכן ללכת בעקבותייך .\n",
            "= i am ready to follow you .\n",
            "< i am ready to follow you . <EOS>\n",
            "\n",
            "> הוא גולש על גלגיליות .\n",
            "= he is skating .\n",
            "< he is skating . <EOS>\n",
            "\n",
            "> הוא לא בביתו .\n",
            "= he is not in .\n",
            "< he is not in the river . <EOS>\n",
            "\n",
            "> הקדמת .\n",
            "= you are early .\n",
            "< you are early and wife . <EOS>\n",
            "\n",
            "> היא אדם אמין .\n",
            "= she is a reliable person .\n",
            "< she is a reliable person . <EOS>\n",
            "\n",
            "> הוא אדם של מעשים .\n",
            "= he is a man of action .\n",
            "< he is a man of action . <EOS>\n",
            "\n",
            "> הוא ילד מופרע .\n",
            "= he is an unmanageable child .\n",
            "< he is an unmanageable child . <EOS>\n",
            "\n",
            "> הוא עסוק מאוד כעת .\n",
            "= he is very busy now .\n",
            "< he is very busy now . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exact-Match Sentence-Level Accuracy Evaluation - not from tutorial just for senity check"
      ],
      "metadata": {
        "id": "AFMUnDzKh81p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateAccuracy(encoder, decoder, pairs, input_lang, output_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYQ7Kuxeh7-Z",
        "outputId": "be7d905c-cf3b-4905-f3a4-688e67f0d711"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of correct pred is: 1123\n",
            "the number of wrong is: 447\n",
            "accuracy: 0.7153\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7152866242038216"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.c. Attention plots 5 random sentences"
      ],
      "metadata": {
        "id": "NOHKbWHOeOYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#matplotlib inline\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "  input = random.choice(pairs)[0]\n",
        "  evaluateAndShowAttention(input)\n",
        "\n"
      ],
      "metadata": {
        "id": "6WH3-U1weRZE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.\n"
      ],
      "metadata": {
        "id": "0xsebFMoNVro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, it is important to note that the evaluations presented above (and in the tutorial) are based only on the training set, which makes the task easier for the model, since it is evaluated on sentences it has already seen during training.\n",
        "\n",
        "Using Exact-Match Sentence-Level Accuracy on the full training set (as implemented in 2.b), the model achieved an accuracy of about 70%. This result is also consistent with the qualitative evaluation on 20 randomly selected sentences. On the one hand, this is an impressive result for a relatively shallow seq2seq model with attention, trained on a very small dataset (1,000–1,500 sentence pairs). The model clearly learns basic sentence structure and common translation patterns.\n",
        "\n",
        "On the other hand, this performance is far from sufficient for a real translation task. Exact sentence matching is a very strict metric, but even qualitatively, many predictions are fluent yet semantically incorrect. This is especially problematic in translation tasks, where users often cannot verify correctness if they do not know the source language. In addition, the vocabulary is extremely limited, so unseen words or slightly different sentence structures cause the model to fail.\n",
        "\n",
        "In addition, the model has clear limitations. The dataset is highly biased by the eng_prefixes filter, which restricts the model to a narrow sentence structure and harms generalization. Moreover, the RNN-based architecture struggles with longer dependencies and scales poorly, and the evaluation is limited since no proper test set or semantic metric is used. Furthermore, based on the attention visualizations, the attention mechanism does not always align cleanly between source and target tokens, suggesting that the learned attention is not sufficiently precise.\n",
        "\n",
        "To improve the model, I would train it on a much larger and more diverse dataset without restrictive prefix filtering. Reducing exposure bias by gradually annealing the teacher forcing ratio would help the model perform more robustly at inference time. In addition, increasing the model capacity (for example, using a larger hidden size or more layers) could improve expressiveness. Finally, replacing the RNN-based architecture with a Transformer-based model or experimenting with alternative attention mechanisms could lead to better alignment and overall translation quality, while pretrained embeddings would improve scalability and vocabulary coverage for translation tasks."
      ],
      "metadata": {
        "id": "e8kxOEYDNe9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4."
      ],
      "metadata": {
        "id": "Kq11yLpwuqm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####New encoder and decoder (Explanation below all)"
      ],
      "metadata": {
        "id": "flepFegRnD9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBiGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    BiGRU encoder (2 layers) that returns:\n",
        "      - encoder_outputs: (B, T, 2*H)\n",
        "      - encoder_hidden_for_decoder: (num_layers, B, H)  (directions combined)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2, dropout_p=0.2, pad_idx=None):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if pad_idx is None:\n",
        "            self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=pad_idx)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # bidirectional=True => outputs are size 2*hidden_size\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_p if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: (B, T)\n",
        "        emb = self.dropout(self.embedding(input_ids))  # (B, T, H)\n",
        "        encoder_outputs, hidden = self.gru(emb)\n",
        "        # encoder_outputs: (B, T, 2H)\n",
        "        # hidden: (num_layers*2, B, H)  (because bidirectional)\n",
        "\n",
        "        # Combine the two directions so decoder can use (num_layers, B, H)\n",
        "        hidden = self._combine_bidir_hidden(hidden)  # (num_layers, B, H)\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "    def _combine_bidir_hidden(self, hidden):\n",
        "        # hidden: (num_layers*2, B, H) -> (num_layers, 2, B, H)\n",
        "        hidden = hidden.view(self.num_layers, 2, hidden.size(1), hidden.size(2))\n",
        "        # simple + stable: sum forward/backward\n",
        "        hidden = hidden[:, 0] + hidden[:, 1]  # (num_layers, B, H)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Additive (Bahdanau) attention for:\n",
        "      query: (B, 1, Hq)\n",
        "      keys:  (B, T, Hk)\n",
        "    returns:\n",
        "      context: (B, 1, Hk)\n",
        "      weights: (B, 1, T)\n",
        "    \"\"\"\n",
        "    def __init__(self, query_size, key_size, attn_size):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Linear(query_size, attn_size, bias=False)\n",
        "        self.Wk = nn.Linear(key_size, attn_size, bias=False)\n",
        "        self.v  = nn.Linear(attn_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # query: (B, 1, Hq), keys: (B, T, Hk)\n",
        "        q = self.Wq(query)          # (B, 1, A)\n",
        "        k = self.Wk(keys)           # (B, T, A)\n",
        "        scores = self.v(torch.tanh(q + k)).squeeze(-1)  # (B, T)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1).unsqueeze(1)  # (B, 1, T)\n",
        "        context = torch.bmm(weights, keys)                # (B, 1, Hk)\n",
        "        return context, weights\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    2-layer GRU decoder with:\n",
        "      - Bahdanau attention over BiGRU encoder outputs (2H)\n",
        "      - input-feeding (previous context is fed into next step)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, output_size, num_layers=2, dropout_p=0.2, pad_idx=None):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if pad_idx is None:\n",
        "            self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=pad_idx)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # encoder key size is 2H (because BiGRU)\n",
        "        self.attention = BahdanauAttention(\n",
        "            query_size=hidden_size,\n",
        "            key_size=2 * hidden_size,\n",
        "            attn_size=hidden_size\n",
        "        )\n",
        "\n",
        "        # input-feeding: [embedded (H) ; prev_context (2H)] => 3H\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=3 * hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_p if num_layers > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # output uses both decoder state and current context: [H ; 2H] => 3H\n",
        "        self.out = nn.Linear(3 * hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, max_length=MAX_LENGTH):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (B, T, 2H)\n",
        "        encoder_hidden:  (num_layers, B, H)  (from EncoderBiGRU)\n",
        "        target_tensor:   (B, T_out) or None\n",
        "        \"\"\"\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        # start token\n",
        "        decoder_input = torch.full(\n",
        "            (batch_size, 1),\n",
        "            SOS_token,\n",
        "            dtype=torch.long,\n",
        "            device=encoder_outputs.device\n",
        "        )\n",
        "\n",
        "        decoder_hidden = encoder_hidden  # (num_layers, B, H)\n",
        "\n",
        "        # input-feeding: start with zero context\n",
        "        prev_context = torch.zeros(\n",
        "            batch_size, 1, 2 * self.hidden_size,\n",
        "            device=encoder_outputs.device\n",
        "        )\n",
        "\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        if target_tensor is not None:\n",
        "            steps = target_tensor.size(1)\n",
        "        else:\n",
        "            steps = max_length\n",
        "\n",
        "        for t in range(steps):\n",
        "            decoder_logits, decoder_hidden, attn_weights, prev_context = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, prev_context\n",
        "            )\n",
        "\n",
        "            decoder_outputs.append(decoder_logits)   # (B,1,V)\n",
        "            attentions.append(attn_weights)          # (B,1,src_len)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, t].unsqueeze(1)  # (B,1)\n",
        "            else:\n",
        "                decoder_input = decoder_logits.argmax(dim=-1).detach()  # (B,1)\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)          # (B,steps,V)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)                    # (B,steps,src_len)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def forward_step(self, input_ids, hidden, encoder_outputs, prev_context):\n",
        "        # input_ids: (B,1)\n",
        "        embedded = self.dropout(self.embedding(input_ids))           # (B,1,H)\n",
        "\n",
        "        # use top layer hidden state as query\n",
        "        query = hidden[-1].unsqueeze(1)                              # (B,1,H)\n",
        "\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)  # context: (B,1,2H)\n",
        "\n",
        "        # input-feeding: concat embedded + prev_context\n",
        "        gru_input = torch.cat([embedded, prev_context], dim=-1)      # (B,1,3H)\n",
        "\n",
        "        output, hidden = self.gru(gru_input, hidden)                 # output: (B,1,H)\n",
        "        # combine output + current context for vocab projection\n",
        "        out_input = torch.cat([output, context], dim=-1)             # (B,1,3H)\n",
        "        logits = self.out(out_input)                                 # (B,1,V)\n",
        "\n",
        "        # update prev_context for next step (classic input-feeding)\n",
        "        prev_context = context\n",
        "\n",
        "        return logits, hidden, attn_weights, prev_context\n"
      ],
      "metadata": {
        "id": "U11ZBjXATgPY"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train"
      ],
      "metadata": {
        "id": "Nxz1UGeFm9v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "enc = EncoderBiGRU(input_lang.n_words, hidden_size).to(device)\n",
        "dec = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, enc, dec, 50, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "Bh0Sx890TsGi",
        "outputId": "686f5cb8-017f-45ea-a122-4cae2c3d6fb6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 136845 sentence pairs\n",
            "Trimmed to 1570 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 1961\n",
            "eng 1199\n",
            "0m 8s (- 1m 16s) (5 10%) 2.2244\n",
            "0m 16s (- 1m 5s) (10 20%) 1.2438\n",
            "0m 23s (- 0m 54s) (15 30%) 0.5820\n",
            "0m 31s (- 0m 46s) (20 40%) 0.2397\n",
            "0m 38s (- 0m 38s) (25 50%) 0.1098\n",
            "0m 46s (- 0m 30s) (30 60%) 0.0634\n",
            "0m 53s (- 0m 23s) (35 70%) 0.0388\n",
            "1m 0s (- 0m 15s) (40 80%) 0.0309\n",
            "1m 8s (- 0m 7s) (45 90%) 0.0233\n",
            "1m 15s (- 0m 0s) (50 100%) 0.0223\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Accuracy and Summary"
      ],
      "metadata": {
        "id": "jyDg3Ux2nZww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateAccuracy(enc, dec, pairs, input_lang, output_lang)"
      ],
      "metadata": {
        "id": "f7DJqWOcusLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0284606-4d5f-405c-d2d2-9f27b52c4c0a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of correct pred is: 1256\n",
            "the number of wrong is: 314\n",
            "accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the new trained model is gain better accuracy of 80% against 70% in the baseline model. The accuracy measure for both is the same and calculated as the number of exact well translated sentences from all sentences.\n",
        "\n",
        "I made few upgraded to the baseline models that helped to get this result. First, I replaced the encoder with a bidirectional GRU, so each source token representation includes information from both past and future context, which gives the attention mechanism richer features to align with. Second, I increased the model capacity by using two GRU layers (with dropout) in the encoder/decoder, allowing the network to learn more complex sequence patterns while reducing overfitting. Third, I updated the Bahdanau attention to attend over the encoder outputs of size\n",
        "2\n",
        "𝐻\n",
        "2H (due to bidirectionality), improving the quality of the context vector at each decoding step. Finally, I added input-feeding in the decoder (feeding the previous context into the next step) and predicted tokens using both the decoder state and the current context, which stabilizes attention and improves translation consistency—together leading to the 80% accuracy."
      ],
      "metadata": {
        "id": "VyDukVvInnHt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "CiwtNgENbx2g",
        "qlMWnOKvQOj1",
        "DeMMqbv3Q4xy",
        "i61HuSercRWM",
        "43N5BbcO6mh4",
        "1JURFKz78MKx",
        "qXJQqZbEbRup",
        "6wvRhhOcgmrQ",
        "AtEJK5IZkk8j",
        "X-d0eIM6FeaM",
        "W7Ci1Ur4LgLY",
        "nqcikDJotuD2",
        "T80C2R8Hx9U-",
        "IpLws2FZG9WN",
        "Vm5wp84OL7Vx",
        "Sjjvdc9zWIT5",
        "EOru_74aXA9a",
        "2uF_R9JAgstu",
        "h4TLaKJhWgBP",
        "40Av3P1EdrPJ",
        "OBkD_Ml8b7GI",
        "hb_GbZwveGck",
        "NOHKbWHOeOYU",
        "0xsebFMoNVro",
        "Kq11yLpwuqm_",
        "flepFegRnD9e",
        "Nxz1UGeFm9v0",
        "jyDg3Ux2nZww"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}